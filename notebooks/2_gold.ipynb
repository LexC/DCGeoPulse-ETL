{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gold: Space Weather — Daily Aggregations\n",
        "\n",
        "This notebook builds the **Gold** layer for the Space Weather pipeline by aggregating **Silver 30-minute windows** into **daily metrics** (UTC).\n",
        "\n",
        "#### What this notebook produces\n",
        "- **Gold Daily (Long)**: one row per **(date_utc, source, metric)** with daily aggregates + coverage/quality diagnostics.\n",
        "- **Gold Daily (Wide, optional)**: one row per **(date_utc, region)** with flattened `<metric_key>_<measure>` columns for analytics/BI.\n",
        "\n",
        "#### Key conventions\n",
        "- **UTC is the only time basis** (session timezone forced to UTC).\n",
        "- `date_utc` is derived from `window_start_utc` (date bucketing in UTC).\n",
        "- Coverage and data-quality signals are computed per day (e.g., overlaps, gaps, midnight spanning)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Libraries & runtime assumptions\n",
        "\n",
        "This notebook runs on **Databricks/Spark** with **Delta Lake** enabled.\n",
        "\n",
        "#### Runtime requirements\n",
        "- A Spark session is available as `spark`\n",
        "- Delta Lake is available (for `DeltaTable` merges/writes)\n",
        "- Session timezone is set to **UTC** (required for correct `date_utc` grouping)\n",
        "\n",
        "#### Timezone note\n",
        "The notebook enforces:\n",
        "- `spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")`\n",
        "\n",
        "This guarantees that `to_date(window_start_utc)` and all midnight-boundary checks behave consistently across clusters and sessions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "from delta.tables import DeltaTable\n",
        "\n",
        "spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Global configuration\n",
        "\n",
        "This section centralizes all runtime settings and storage paths used throughout the notebook.\n",
        "\n",
        "#### What lives here\n",
        "- **Delta paths** for Silver inputs and Gold outputs\n",
        "- **Pipeline constants** (e.g., expected window size, allowed ranges)\n",
        "- **Runtime settings** that must be stable for correctness (notably **UTC timezone**)\n",
        "\n",
        "#### Why it matters\n",
        "Keeping configuration in one place makes the notebook:\n",
        "- easier to review in PRs\n",
        "- safer to run across dev/prod workspaces\n",
        "- less error-prone when paths change\n",
        "\n",
        "> Rule of thumb: if a value changes between environments, it belongs in `VAR` (or environment-specific config), not hard-coded inside logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "def global_variables():\n",
        "    tiers = [\"bronze\", \"silver\", \"gold\"]\n",
        "    container = {tier: f\"abfss://{tier}@alexccrv0dcn.dfs.core.windows.net\" for tier in tiers}\n",
        "\n",
        "    VAR = {\n",
        "        \"container\": container,\n",
        "        \"silver_input\": \"/\".join([container[\"silver\"], \"space_weather\"]),\n",
        "        \"gold_output_long\": \"/\".join([container[\"gold\"], \"space_weather_daily_long\"]),\n",
        "        \"wide_measures\": [\n",
        "            \"value_mean\",\n",
        "            \"value_max\",\n",
        "            \"value_min\",\n",
        "            \"value_stddev\",\n",
        "            \"count_window\",\n",
        "            \"coverage_pct\",\n",
        "        ],\n",
        "        \"regions\": {\n",
        "            \"CH\": {\n",
        "                \"main\":\"JUNG1\",\n",
        "                \"references\": [\"OULU\",\"ROME\"]\n",
        "            }\n",
        "        },\n",
        "    }\n",
        "    return VAR\n",
        "\n",
        "VAR = global_variables()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helpers\n",
        "\n",
        "Utility functions used across the notebook to keep the main logic readable and consistent.\n",
        "\n",
        "#### What helpers cover\n",
        "- **Delta table checks** (e.g., “path exists but is not a Delta table”)\n",
        "- **Safe read/write patterns** for idempotent pipelines\n",
        "- **Small reusable transforms** that are not business logic (formatting, validation, etc.)\n",
        "\n",
        "#### Design principles\n",
        "- Helpers should be **pure** where possible (same input → same output).\n",
        "- Prefer **explicit errors** over silent failure for invalid table states.\n",
        "- Keep business rules out of helpers (those belong in the transform functions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "def assert_delta_or_missing(path: str) -> None:\n",
        "    \"\"\"\n",
        "    Assert that a storage path is either a Delta table, or does not exist.\n",
        "\n",
        "    Args:\n",
        "        path (str): Storage path to validate.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If the path exists but is not a Delta table.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- SETUP AND VALIDATION ---\n",
        "    if not path or not path.strip():\n",
        "        raise ValueError(\"path must be a non-empty string.\")\n",
        "\n",
        "    # --- LOGIC ---\n",
        "    # If it's delta, we're good.\n",
        "    try:\n",
        "        if DeltaTable.isDeltaTable(spark, path):\n",
        "            return\n",
        "    except Exception:\n",
        "        # If isDeltaTable itself fails (e.g., permissions / transient FS issues),\n",
        "        # fall back to existence check below to decide what to do.\n",
        "        pass\n",
        "\n",
        "    # If path doesn't exist, we're good.\n",
        "    try:\n",
        "        listing = dbutils.fs.ls(path)\n",
        "    except Exception:\n",
        "        return\n",
        "\n",
        "    # Path exists but is not delta.\n",
        "    raise ValueError(f\"Path exists but is not a Delta table: {path}\")\n",
        "\n",
        "\n",
        "def upsert_delta(df: DataFrame, path: str, keys: List[str]) -> None:\n",
        "    ### HAS TO CHANGE TO ONLY APPEND INFO THAT DOES NOT ALREDY EXISTS\n",
        "    \"\"\"\n",
        "    Upsert a DataFrame into a Delta Lake table at the given path.\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): Source DataFrame to upsert.\n",
        "        path (str): Delta table path.\n",
        "        keys (List[str]): Column names that define the merge key.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If `keys` is empty, or the path exists but is not a Delta table.\n",
        "    \"\"\"\n",
        "\n",
        "    # --- SETUP AND VALIDATION ---\n",
        "    assert_delta_or_missing(path)\n",
        "\n",
        "    if not keys:\n",
        "        raise ValueError(\"keys must be a non-empty list of merge key column names.\")\n",
        "\n",
        "    # --- LOGIC ---\n",
        "    if DeltaTable.isDeltaTable(spark, path):\n",
        "        delta = DeltaTable.forPath(spark, path)\n",
        "\n",
        "        merge_condition = \" AND \".join([f\"t.`{k}` = s.`{k}`\" for k in keys])\n",
        "\n",
        "        (\n",
        "            delta.alias(\"t\")\n",
        "            .merge(df.alias(\"s\"), merge_condition)\n",
        "            .whenMatchedUpdateAll()\n",
        "            .whenNotMatchedInsertAll()\n",
        "            .execute()\n",
        "        )\n",
        "        return\n",
        "\n",
        "    (\n",
        "        df.write.format(\"delta\")\n",
        "        .mode(\"overwrite\")\n",
        "        .save(path)\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gold transforms\n",
        "\n",
        "This notebook builds Gold in two steps:\n",
        "\n",
        "#### 1) Gold Daily (Long)\n",
        "`build_gold_daily_long(silver_df)` aggregates 30-minute Silver windows into one row per:\n",
        "\n",
        "- **Keys:** `date_utc, source, metric` (UTC)\n",
        "- **Aggregates:** daily statistics derived from `value`\n",
        "- **Coverage & diagnostics:** derived from `window_start_utc + window_duration_s`, including:\n",
        "  - window coverage within the day\n",
        "  - overlaps / gaps between windows\n",
        "  - whether any window spans **past midnight UTC** (and how many seconds)\n",
        "\n",
        "The long table is the canonical daily output and is the input to the wide build.\n",
        "\n",
        "#### 2) Gold Daily (Wide, per region)\n",
        "`build_gold_daily_wide_by_region(gold_long_df, region, nmdb_stations)` creates a single-row-per-day table for one region:\n",
        "\n",
        "- **Keys:** `date_utc, region`\n",
        "- **Metric selection:**\n",
        "  - keeps **all non-NMDB** metrics (e.g., GFZ)\n",
        "  - keeps **only selected NMDB stations** defined by:\n",
        "    `{\"main\": \"<CODE>\", \"references\": [\"<CODE1>\", ...]}`\n",
        "- **Quality filters (wide is “clean-only”):**\n",
        "  - keeps only homogeneous days (`is_window_duration_homogeneous`)\n",
        "  - drops days with overlaps (`has_overlap`)\n",
        "  - drops days with midnight-spanning windows (`has_window_spanning_midnight`)\n",
        "- **Stable NMDB aliases:**\n",
        "  - main station → `metric = \"main\"`\n",
        "  - reference stations → `metric = \"reference1\"`, `reference2`, ...\n",
        "\n",
        "The wide table is optimized for analytics/BI and uses a normalized `metric_key` to produce `<metric_key>_<measure>` columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_gold_daily_long(silver_df: DataFrame) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Aggregate windowed Silver metrics into daily (UTC) Gold rows with coverage and\n",
        "    interval-quality diagnostics.\n",
        "\n",
        "    Computes daily stats (mean/max/min/stddev), coverage, window-duration metadata,\n",
        "    within-day gap/overlap diagnostics, plus a cross-midnight spillover flag and the\n",
        "    number of seconds a window extends past midnight.\n",
        "\n",
        "    Notes:\n",
        "        - `has_window_spanning_midnight` is true if any window for that day ends after\n",
        "          00:00:00 of the next day (relative to its start date_utc).\n",
        "        - `crossed_s` is the maximum spillover seconds among windows for that day.\n",
        "          Example: 23:30 -> 00:15 yields crossed_s = 900.\n",
        "\n",
        "    Args:\n",
        "        silver_df (DataFrame): Input Spark DataFrame with columns:\n",
        "            window_start_utc, source, metric, value, window_duration_s.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Daily Gold DataFrame keyed by [date_utc, source, metric].\n",
        "    \"\"\"\n",
        "\n",
        "    # --- SETUP AND VALIDATION ---\n",
        "    day_s = 24 * 60 * 60\n",
        "    keys = [\"date_utc\", \"source\", \"metric\"]\n",
        "\n",
        "    base = (\n",
        "        silver_df\n",
        "            .select(\"window_start_utc\", \"source\", \"metric\", \"value\", \"window_duration_s\")\n",
        "            .filter(F.col(\"window_start_utc\").isNotNull())\n",
        "            .filter(F.col(\"window_duration_s\").isNotNull())\n",
        "            .withColumn(\"date_utc\", F.to_date(\"window_start_utc\"))\n",
        "    )\n",
        "\n",
        "    valid = base.filter(F.col(\"value\").isNotNull())\n",
        "    \n",
        "    # --- LOGIC ---\n",
        "\n",
        "    # Compute explicit interval end\n",
        "    checks = valid.withColumn(\n",
        "        \"window_end_utc\",\n",
        "        F.to_timestamp(\n",
        "            F.from_unixtime(\n",
        "                F.col(\"window_start_utc\").cast(\"long\") + F.col(\"window_duration_s\")\n",
        "            )\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    # Cross-midnight spillover (how much the window extends past next midnight)\n",
        "    checks = (\n",
        "        checks\n",
        "        .withColumn(\"next_midnight_utc\",\n",
        "            F.to_timestamp(\n",
        "                F.concat_ws(\n",
        "                        \" \",\n",
        "                        F.date_add(F.col(\"date_utc\"), 1).cast(\"string\"),\n",
        "                        F.lit(\"00:00:00\"),\n",
        "                        )\n",
        "                    )\n",
        "                )\n",
        "        .withColumn(\n",
        "            \"crossed_s_row\",\n",
        "            F.greatest(\n",
        "                F.lit(0),\n",
        "                F.col(\"window_end_utc\").cast(\"long\")\n",
        "                - F.col(\"next_midnight_utc\").cast(\"long\"),\n",
        "            ),\n",
        "        )\n",
        "        .withColumn(\"has_window_spanning_midnight_row\", F.col(\"crossed_s_row\") > 0)\n",
        "    )\n",
        "\n",
        "    # Within-day gap/overlap checks (per date/source/metric)\n",
        "    w_day_order = Window.partitionBy(*keys).orderBy(F.col(\"window_start_utc\").asc())\n",
        "    w_day_prev = w_day_order.rowsBetween(Window.unboundedPreceding, -1)\n",
        "\n",
        "    checks = (\n",
        "        checks.withColumn(\"prev_max_end_utc_day\", F.max(\"window_end_utc\").over(w_day_prev))\n",
        "        .withColumn(\n",
        "            \"has_overlap_row\",\n",
        "            F.col(\"prev_max_end_utc_day\").isNotNull()\n",
        "            & (F.col(\"window_start_utc\") < F.col(\"prev_max_end_utc_day\")),\n",
        "        )\n",
        "        .withColumn(\n",
        "            \"has_gap_row\",\n",
        "            F.col(\"prev_max_end_utc_day\").isNotNull()\n",
        "            & (F.col(\"window_start_utc\") > F.col(\"prev_max_end_utc_day\")),\n",
        "        )\n",
        "        .withColumn(\n",
        "            \"overlap_s\",\n",
        "            F.when(\n",
        "                F.col(\"has_overlap_row\"),\n",
        "                F.col(\"prev_max_end_utc_day\").cast(\"long\") - F.col(\"window_start_utc\").cast(\"long\"),\n",
        "            ),\n",
        "        )\n",
        "        .withColumn(\n",
        "            \"gap_s\",\n",
        "            F.when(\n",
        "                F.col(\"has_gap_row\"),\n",
        "                F.col(\"window_start_utc\").cast(\"long\") - F.col(\"prev_max_end_utc_day\").cast(\"long\"),\n",
        "            ),\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "    daily = (\n",
        "        checks.groupBy(*keys)\n",
        "        .agg(\n",
        "            # --- Daily stats ---\n",
        "            F.avg(\"value\").alias(\"value_mean\"),\n",
        "            F.max(\"value\").alias(\"value_max\"),\n",
        "            F.min(\"value\").alias(\"value_min\"),\n",
        "            F.stddev_samp(\"value\").alias(\"value_stddev\"),\n",
        "            # --- Counts / coverage ---\n",
        "            F.count(\"*\").alias(\"count_window\"),\n",
        "            F.sum(\"window_duration_s\").alias(\"covered_window_s\"),\n",
        "            # --- Window metadata ---\n",
        "            F.sort_array(F.collect_set(\"window_duration_s\")).alias(\"window_duration_s_values\"),\n",
        "            # --- Interval diagnostics (within-day) ---\n",
        "            F.max(F.col(\"has_overlap_row\").cast(\"int\")).cast(\"boolean\").alias(\"has_overlap\"),\n",
        "            F.max(F.col(\"has_gap_row\").cast(\"int\")).cast(\"boolean\").alias(\"has_gap\"),\n",
        "            F.max(F.coalesce(F.col(\"overlap_s\"), F.lit(0))).alias(\"max_overlap_s\"),\n",
        "            F.max(F.coalesce(F.col(\"gap_s\"), F.lit(0))).alias(\"max_gap_s\"),\n",
        "            # --- Cross-midnight spillover (what you asked for) ---\n",
        "            F.max(F.col(\"has_window_spanning_midnight_row\").cast(\"int\"))\n",
        "                .cast(\"boolean\").alias(\"has_window_spanning_midnight\"),\n",
        "            F.max(\"crossed_s_row\").cast(\"long\").alias(\"crossed_s\"),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    daily = (\n",
        "        daily.withColumn(\"is_window_duration_homogeneous\", F.size(\"window_duration_s_values\") == 1)\n",
        "        .withColumn(\n",
        "            \"window_duration_s\",\n",
        "            F.when(F.col(\"is_window_duration_homogeneous\"), F.element_at(\"window_duration_s_values\", 1)),\n",
        "        )\n",
        "        .withColumn(\"coverage_pct\", F.col(\"covered_window_s\") / F.lit(day_s))\n",
        "    )\n",
        "\n",
        "    return daily.select(\n",
        "        # --- Keys ---\n",
        "        *keys,\n",
        "        # --- Daily stats ---\n",
        "        \"value_mean\",\n",
        "        \"value_max\",\n",
        "        \"value_min\",\n",
        "        \"value_stddev\",\n",
        "        # --- Counts / coverage ---\n",
        "        \"count_window\",\n",
        "        \"covered_window_s\",\n",
        "        \"coverage_pct\",\n",
        "        # --- Window metadata ---\n",
        "        \"window_duration_s_values\",\n",
        "        \"is_window_duration_homogeneous\",\n",
        "        \"window_duration_s\",\n",
        "        # --- Interval diagnostics (within-day) ---\n",
        "        \"has_overlap\",\n",
        "        \"has_gap\",\n",
        "        \"max_overlap_s\",\n",
        "        \"max_gap_s\",\n",
        "        # --- Cross-midnight spillover ---\n",
        "        \"has_window_spanning_midnight\",\n",
        "        \"crossed_s\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_metric_key(df: DataFrame) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Create a normalized join-friendly metric key from `source` and `metric`.\n",
        "\n",
        "    The key is built as `lower(source) + \"_\" + lower(metric)`, then normalized by:\n",
        "    - Replacing any non-alphanumeric characters with underscores.\n",
        "    - Collapsing repeated underscores to a single underscore.\n",
        "    - Stripping leading/trailing underscores.\n",
        "\n",
        "    Args:\n",
        "        df (DataFrame): Input Spark DataFrame containing `source` and `metric`\n",
        "            columns.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: A new DataFrame with an additional `metric_key` column.\n",
        "    \"\"\"\n",
        "    metric_key = F.concat_ws(\"_\", F.lower(F.col(\"source\")), F.lower(F.col(\"metric\")))\n",
        "    metric_key = F.regexp_replace(metric_key, r\"[^a-z0-9]+\", \"_\")\n",
        "    metric_key = F.regexp_replace(metric_key, r\"_+\", \"_\")\n",
        "    metric_key = F.regexp_replace(metric_key, r\"^_+|_+$\", \"\")\n",
        "    return df.withColumn(\"metric_key\", metric_key)\n",
        "\n",
        "def build_gold_daily_wide_by_region(\n",
        "    long_df: DataFrame, region: str, nmdb_stations: dict) -> DataFrame:\n",
        "    \"\"\"\n",
        "    Build a wide daily table for one region using:\n",
        "      - all non-NMDB metrics (e.g., GFZ), plus\n",
        "      - NMDB metrics restricted to the provided station spec.\n",
        "\n",
        "    The NMDB station codes are remapped to stable aliases:\n",
        "      - nmdb_stations[\"main\"] -> metric = \"main\"\n",
        "      - nmdb_stations[\"references\"][i] -> metric = f\"reference{i+1}\"\n",
        "\n",
        "    Args:\n",
        "        long_df (DataFrame): Long daily table keyed by [date_utc, source, metric].\n",
        "        region (str): Region label to attach to the output.\n",
        "        nmdb_stations (dict): Station spec:\n",
        "            {\"main\": \"<CODE>\", \"references\": [\"<CODE1>\", \"<CODE2>\", ...]}.\n",
        "\n",
        "    Returns:\n",
        "        DataFrame: Wide daily table with date_utc, region, and <metric_key>_<measure> columns.\n",
        "    \"\"\"\n",
        "    # --- SETUP / VALIDATION ---\n",
        "    main_station = (nmdb_stations.get(\"main\") or \"\").strip()\n",
        "    if not main_station:\n",
        "        raise ValueError('nmdb_stations[\"main\"] must be a non-empty station code.')\n",
        "\n",
        "    ref_stations = nmdb_stations.get(\"references\") or []\n",
        "\n",
        "    main_norm = main_station.upper()\n",
        "    refs_norm = [\n",
        "        (s or \"\").strip().upper()\n",
        "        for s in ref_stations\n",
        "        if s is not None and str(s).strip()\n",
        "    ]\n",
        "    station_codes = [main_norm] + refs_norm\n",
        "\n",
        "    # --- FILTERS (QUALITY + NMDB SELECTION) ---\n",
        "    filtered = (\n",
        "        long_df\n",
        "            .filter((F.col(\"source\") != \"NMDB\") |\n",
        "                    (F.upper(F.col(\"metric\")).isin(station_codes)))\n",
        "            .filter(F.col(\"is_window_duration_homogeneous\"))\n",
        "            .filter(~F.col(\"has_overlap\"))\n",
        "            .filter(~F.col(\"has_window_spanning_midnight\"))\n",
        "    )\n",
        "\n",
        "    # --- NMDB REMAP: STATION_CODE -> alias (\"main\", \"reference1\", ...) ---\n",
        "    kvs = [F.lit(main_norm), F.lit(\"main\")]\n",
        "    for i, code in enumerate(refs_norm, start=1):\n",
        "        kvs.extend([F.lit(code), F.lit(f\"reference{i}\")])\n",
        "\n",
        "    mapping_expr = F.create_map(*kvs)\n",
        "\n",
        "    # Rename NMDB metrics to stable aliases before metric_key is created.\n",
        "    # coalesce() is defensive: if mapping is missing for any reason, keep original metric.\n",
        "    filtered = filtered.withColumn(\n",
        "        \"metric\",\n",
        "        F.when(\n",
        "            F.col(\"source\") == \"NMDB\",\n",
        "            F.coalesce(\n",
        "                mapping_expr[F.upper(F.col(\"metric\"))],\n",
        "                F.col(\"metric\"),\n",
        "            ),\n",
        "        ).otherwise(F.col(\"metric\")),\n",
        "    )\n",
        "    \n",
        "    # --- METRIC KEYS ---\n",
        "    df = add_metric_key(filtered)\n",
        "    \n",
        "    # --- SINGLE PIVOT USING STRUCT OF MEASURES ---\n",
        "    measures = list(VAR[\"wide_measures\"])\n",
        "    payload = F.struct(*[F.col(m).alias(m) for m in measures])\n",
        "\n",
        "    pivoted = (\n",
        "        df.select(\"date_utc\", \"metric_key\", payload.alias(\"payload\"))\n",
        "        .groupBy(\"date_utc\")\n",
        "        .pivot(\"metric_key\")\n",
        "        .agg(F.first(\"payload\"))\n",
        "    )\n",
        "\n",
        "    # --- EXPAND STRUCT COLUMNS INTO <metric_key>_<measure> ---\n",
        "    select_exprs = [F.col(\"date_utc\")]\n",
        "    for c in pivoted.columns:\n",
        "        if c == \"date_utc\":\n",
        "            continue\n",
        "        # c is a metric_key column containing a struct(payload)\n",
        "        for m in measures:\n",
        "            select_exprs.append(F.col(f\"`{c}`.{m}\").alias(f\"{c}_{m}\"))\n",
        "\n",
        "    wide = pivoted.select(*select_exprs).withColumn(\"region\", F.lit(region))\n",
        "    \n",
        "    # --- RETURN ---\n",
        "    base_cols = [\"date_utc\", \"region\"]\n",
        "    data_cols = [c for c in wide.columns if c not in (\"date_utc\", \"region\")]\n",
        "    return wide.select(*base_cols, *data_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Show Time\n",
        "\n",
        "Execution flow:\n",
        "\n",
        "1. **Load Silver**  \n",
        "   Read the Silver Delta table from `VAR[\"silver_input\"]`.\n",
        "\n",
        "2. **Build Gold Daily (Long)**  \n",
        "   Compute `gold_long_df = build_gold_daily_long(silver_df)`.\n",
        "\n",
        "3. **Write / upsert Gold Long (Delta)**  \n",
        "   Merge into `VAR[\"gold_output_long\"]` using keys: `[\"date_utc\", \"source\", \"metric\"]`.\n",
        "\n",
        "4. **Build + write Gold Daily (Wide) per region**  \n",
        "   For each `(region, stations)` in `VAR[\"regions\"]`:\n",
        "   - `gold_wide_df = build_gold_daily_wide_by_region(gold_long_df, region, stations)`\n",
        "   - upsert into: `abfss://gold@.../space_weather_daily_<region>`  \n",
        "     using keys: `[\"date_utc\", \"region\"]`\n",
        "\n",
        "Note: `upsert_delta()` performs a true upsert (matched keys are updated, non-matched keys are inserted).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {},
      "outputs": [],
      "source": [
        "silver_path = VAR[\"silver_input\"]\n",
        "gold_long_path = VAR[\"gold_output_long\"]\n",
        "\n",
        "silver_df = spark.read.format(\"delta\").load(silver_path)\n",
        "\n",
        "gold_long_df = build_gold_daily_long(silver_df)\n",
        "upsert_delta(gold_long_df, gold_long_path, keys=[\"date_utc\", \"source\", \"metric\"])\n",
        "\n",
        "for region, stations in VAR[\"regions\"].items():\n",
        "    gold_wide_path = \"/\".join([VAR[\"container\"][\"gold\"], f\"space_weather_daily_{region}\"])\n",
        "    gold_wide_df = build_gold_daily_wide_by_region(gold_long_df, region, stations)\n",
        "    upsert_delta(gold_wide_df, gold_wide_path, keys=[\"date_utc\", \"region\"])\n"
      ]
    }
  ],
  "metadata": {
    "application/vnd.databricks.v1+notebook": {
      "computePreferences": null,
      "dashboards": [],
      "environmentMetadata": {
        "base_environment": "",
        "environment_version": "4"
      },
      "inputWidgetPreferences": null,
      "language": "python",
      "notebookMetadata": {
        "pythonIndentUnit": 4
      },
      "notebookName": "2_gold",
      "widgets": {}
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
