{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edd9e530-b934-46f6-b5a3-6dbed8d5ee2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Silver: Space Weather (GFZ + NMDB)\n",
    "\n",
    "This notebook builds the **Silver** table for the Space Weather domain by transforming **Bronze** extracts (created by `0_bronze.ipynb`) into a **normalized, windowed, long-form** dataset.\n",
    "\n",
    "#### What “30-minute windows” means here\n",
    "Each row represents a metric value computed over a fixed time window:\n",
    "- `window_start_utc` = the **UTC start timestamp** of the window\n",
    "- `window_duration_s` = the **window length in seconds** (constant `1800`, i.e., 30 minutes)\n",
    "- The window covers `[window_start_utc, window_start_utc + 1800s)`\n",
    "\n",
    "This is important because **multiple sources and metrics** share the same temporal grid, enabling consistent downstream aggregations (daily averages, joins, anomaly checks, etc.).\n",
    "\n",
    "#### Inputs\n",
    "- **GFZ** Bronze files (geomagnetic indices; currently focused on `Hp30`)\n",
    "- **NMDB** Bronze files (station readings provided as raw ASCII lines; parsed via regex)\n",
    "\n",
    "#### Output (Silver schema)\n",
    "| column | type | meaning |\n",
    "|---|---|---|\n",
    "| `window_start_utc` | timestamp | window start (UTC) |\n",
    "| `source` | string | `GFZ` or `NMDB` |\n",
    "| `metric` | string | index/station identifier (e.g., `Hp30`, `OULU`) |\n",
    "| `value` | double | numeric value for that window |\n",
    "| `window_duration_s` | int | window size in seconds (constant `1800`) |\n",
    "\n",
    "#### Processing overview\n",
    "1. Read Bronze files per configured task (`VAR[\"bronze_files\"]`)\n",
    "2. Parse + normalize into the common schema above\n",
    "3. Apply data-quality rules (nulling invalid values, collapsing duplicates, avoiding re-inserts)\n",
    "4. Append into the Silver Delta destination (`VAR[\"silver_output\"]`)\n",
    "\n",
    "#### How to run / configure\n",
    "- Attach to a Databricks cluster (Spark + Delta available).\n",
    "- Edit `global_variables()`:\n",
    "  - set your storage/container paths (if needed)\n",
    "  - provide one Bronze file path per task in `bronze_files` (missing/blank entries are skipped)\n",
    "- Run all cells top-to-bottom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7e136d3-2b01-4a5d-93d0-19b32f24f36f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Libraries & runtime assumptions\n",
    "\n",
    "This notebook intentionally keeps dependencies minimal:\n",
    "\n",
    "- **Standard library**: lightweight parsing and helpers (e.g., regex).\n",
    "- **Spark (PySpark)**: all transformations are executed as Spark DataFrame operations.\n",
    "- **Delta Lake**: the Silver output is written as a **Delta** dataset.\n",
    "\n",
    "**Assumes Databricks runtime** (i.e., an active `spark` session and Delta support available on the cluster).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ebf414c-1129-4d64-961d-605e5106fb61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f2d138c-c24f-4d81-b33d-3aafdfedf9a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Pipeline building blocks\n",
    "\n",
    "This section defines the core functions that the notebook wires together later. The cell at the end should read like a short recipe because the heavy lifting lives here.\n",
    "\n",
    "#### What’s defined below\n",
    "\n",
    "- **Persistence helpers**\n",
    "  Ensure the Silver Delta destination exists and write/append in a re-runnable way.\n",
    "\n",
    "- **Source readers (Bronze → Silver shape)**\n",
    "  GFZ and NMDB parsing/shaping logic that produces the long-form, windowed table.\n",
    "\n",
    "- **Schema normalization**\n",
    "  Standardize timestamps and column naming so all sources converge to:\n",
    "  `window_start_utc, source, metric, value, window_duration_s`.\n",
    "\n",
    "- **Data-quality transforms**\n",
    "  Small Spark transforms that enforce deterministic rules (e.g., null known-invalid values, collapse duplicates) without changing the schema.\n",
    "\n",
    "#### Design principles\n",
    "- **One job per function** (easy to test, easy to chain).\n",
    "- **Spark-first** (DataFrame transforms, no hidden side effects).\n",
    "- **Deterministic + rerunnable** (re-execution shouldn’t create inconsistent Silver output).\n",
    "\n",
    "Flow later is intentionally boring: configure → read → normalize → (DQ rules) → write."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1828e121-96a0-4b0c-8e2e-c1b90d48f104",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Global configuration\n",
    "\n",
    "All notebook settings live in `VAR = global_variables()`:\n",
    "\n",
    "- **Paths**: Bronze/Silver roots and the final `silver_output`\n",
    "- **What to run**: `tasks` (ordered)\n",
    "- **Inputs**: `bronze_files` mapping task → Bronze file path (missing/blank paths are skipped)\n",
    "- **Constants / rules**: e.g., `window_duration_s = 1800`, allowed NMDB stations, value ranges\n",
    "\n",
    "Change this cell when you switch environments or adjust scope—functions should not hardcode paths or task lists.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c40be9fd-078b-44be-b48a-2395da8028b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def global_variables():\n",
    "    \"\"\"Build the global configuration dictionary used throughout the notebook.\n",
    "\n",
    "    Returns:\n",
    "        dict: The VAR configuration dictionary (paths, ranges, reader functions, etc.).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # This will work in a Jobs run (taskValues are only available there)\n",
    "        bronze_output = dbutils.jobs.taskValues.get(taskKey=\"Bronze\", key=\"bronze_output\")\n",
    "        container = bronze_output.get(\"container\", \"\")\n",
    "        bronze_files = bronze_output.get(\"output_bronze\", \"\")\n",
    "    except Exception:\n",
    "        # Not in a Jobs run (e.g., interactive notebook) or task value not available, for testing\n",
    "        tiers = [\"bronze\", \"silver\", \"gold\"]\n",
    "        container = {tier: f\"abfss://{tier}@alexccrv0dcn.dfs.core.windows.net\" for tier in tiers}\n",
    "        bronze_files = {\n",
    "            \n",
    "            }\n",
    "\n",
    "    VAR = {\n",
    "        \"container\": container,\n",
    "        \"bronze_files\": bronze_files,\n",
    "        \"silver_output\": \"/\".join([container[\"silver\"], \"space_weather\"]),\n",
    "        \"gfz_hp30_range\": (0.0, 9.0),\n",
    "        \"window_duration_s\": 1800,\n",
    "    }\n",
    "    return VAR\n",
    "VAR = global_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6f9ef10-c7ca-491a-91c0-686849e696b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Helpers\n",
    "\n",
    "Utility functions that make the run safe and repeatable:\n",
    "\n",
    "- Ensure the **Silver Delta** destination exists with the expected schema\n",
    "\n",
    "This keeps the main flow clean: configure → ensure destination → transform → write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3262a80-6c1b-461e-b703-e66b5f16bf31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def ensure_silver_delta():\n",
    "    \"\"\"Ensure the Silver Delta destination exists and is initialized.\n",
    "\n",
    "    Creates the Delta location defined by VAR[\"silver_output\"] if missing and initializes\n",
    "    an empty Delta table with the expected schema.\n",
    "    \"\"\"\n",
    "    silver_path = VAR[\"silver_output\"]\n",
    "\n",
    "    schema = T.StructType([\n",
    "        T.StructField(\"window_start_utc\", T.TimestampType(), True),\n",
    "        T.StructField(\"source\", T.StringType(), True),\n",
    "        T.StructField(\"metric\", T.StringType(), True),\n",
    "        T.StructField(\"value\", T.DoubleType(), True),\n",
    "        T.StructField(\"window_duration_s\", T.IntegerType(), True),\n",
    "    ])\n",
    "\n",
    "    # 1) If it's already a Delta table, we're done\n",
    "    if DeltaTable.isDeltaTable(spark, silver_path):\n",
    "        return\n",
    "\n",
    "    # 2) If path exists but isn't Delta, fail (safety)\n",
    "    try:\n",
    "        dbutils.fs.ls(silver_path)\n",
    "    except Exception:\n",
    "        # Doesn't exist (or not listable) → create empty Delta table\n",
    "        (\n",
    "            spark.createDataFrame([], schema)\n",
    "            .write.format(\"delta\")\n",
    "            .mode(\"overwrite\")\n",
    "            .save(silver_path)\n",
    "        )\n",
    "        return\n",
    "\n",
    "    # Exists but not Delta\n",
    "    raise ValueError(f\"Path exists but is not a Delta table: {silver_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58157386-77d5-4aad-88c8-b8b70e8187f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Read Bronze: GFZ\n",
    "\n",
    "Loads the **GFZ** Bronze extract and converts it into the standard Silver-ready shape (windowed, long-form).\n",
    "\n",
    "- Input: Bronze GFZ file(s) for the configured task(s)\n",
    "- Output columns: `window_start_utc, source, metric, value, window_duration_s`\n",
    "- Notes: focuses on the configured geomagnetic index (e.g., `Hp30`) and preserves the 30-minute window grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41300557-7fc8-4bc8-87a4-77705f3f1079",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def readbc_gfz(gfz_path: str) -> DataFrame:\n",
    "    \"\"\"Read one GFZ Bronze file and return canonical Silver-shaped rows.\n",
    "\n",
    "    Args:\n",
    "        gfz_path (str): Bronze input path for a GFZ export.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Canonical long-form rows with columns:\n",
    "            window_start_utc, source, metric, value, window_duration_s.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the metric cannot be inferred from the file path or the\n",
    "            input schema is not as expected.\n",
    "    \"\"\"\n",
    "    index_pattern = r\"_index-([^_]+)_\"\n",
    "    UTCZ_pattern  = r\"^(\\d{4}-\\d{2}-\\d{2})T(\\d{2}:\\d{2}:\\d{2})Z$\"\n",
    "    UTCZ_replace  = r\"$1 $2\"\n",
    "\n",
    "    if not index_pattern:\n",
    "        raise ValueError(f\"Could not extract GFZ index from path: {gfz_path}\")\n",
    "\n",
    "    idx = re.search(index_pattern, gfz_path).group(1)\n",
    "    \n",
    "    df = spark.read.option(\"header\", \"true\").csv(gfz_path)\n",
    "    value_cols = [c for c in df.columns if c != \"datetime\"]\n",
    "    if len(value_cols) != 1:\n",
    "        raise ValueError(\n",
    "            f\"Expected one value column in GFZ CSV for {idx}.\"\n",
    "        )\n",
    "\n",
    "    df = (\n",
    "        df.withColumnRenamed(\"datetime\", \"window_start_utc\")\n",
    "            .withColumn(\"window_start_utc\",  F.regexp_replace(\"window_start_utc\", UTCZ_pattern, UTCZ_replace))\n",
    "            .withColumn(\"window_start_utc\",  F.to_timestamp(F.col(\"window_start_utc\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "            .withColumn(\"source\",F.lit(\"GFZ\"))\n",
    "            .withColumn(\"metric\",F.lit(idx))\n",
    "            .withColumnRenamed(value_cols[0], \"value\")\n",
    "            .withColumn(\"window_duration_s\",F.lit(VAR[\"window_duration_s\"]))\n",
    "            .withColumn(\"value\",F.col(\"value\").cast(\"double\"))\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47c6d4b4-ad7b-4c51-881a-25939f00b753",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Read Bronze: NMDB\n",
    "\n",
    "Loads the **NMDB** Bronze extract (raw ASCII lines) and parses it into the standard Silver-ready shape.\n",
    "\n",
    "- Input: Bronze NMDB file(s) with raw text lines\n",
    "- Parsing: regex-based extraction of timestamp + station values\n",
    "- Output columns: `window_start_utc, source, metric, value, window_duration_s`\n",
    "- Notes: `metric` is the **station code** (e.g., `OULU`); only configured/allowed stations are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18d88a50-0fa2-495b-8e50-02bcdc10e52e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def readbc_nmdb(nmdb_path: str) -> DataFrame:\n",
    "    \"\"\"Read one NMDB Bronze file and return canonical Silver-shaped rows.\n",
    "\n",
    "    Args:\n",
    "        nmdb_path (str): Bronze input path for an NMDB station export.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Canonical long-form rows with columns:\n",
    "            window_start_utc, source, metric, value, window_duration_s.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the station/metric cannot be inferred from the file path or\n",
    "            the ASCII payload cannot be parsed.\n",
    "    \"\"\"\n",
    "    station_pattern = r\"_stations-([^_]+)_\"\n",
    "    NMDB_LINE_PATTERN = r\"^\\s*(\\d{4}-\\d{2}-\\d{2}\\s+\\d{2}:\\d{2}:\\d{2})\\s*[;\\s]+\\s*(-?\\d+(?:\\.\\d+)?)\"\n",
    "\n",
    "    station = re.search(station_pattern, nmdb_path).group(1)\n",
    "\n",
    "    sourcefile = spark.read.text(nmdb_path).withColumnRenamed(\"value\", \"line\")\n",
    "\n",
    "    parsed = (\n",
    "        sourcefile\n",
    "        .withColumn(\"window_start_utc\",  F.regexp_extract(F.col(\"line\"), NMDB_LINE_PATTERN, 1))\n",
    "        .withColumn(\"value\",  F.regexp_extract(F.col(\"line\"), NMDB_LINE_PATTERN, 2))\n",
    "        .filter((F.col(\"window_start_utc\").isNotNull()) & (F.col(\"window_start_utc\") != \"\"))\n",
    "        .filter((F.col(\"value\").isNotNull()) & (F.col(\"value\") != \"\"))\n",
    "    )\n",
    "   \n",
    "    df = (\n",
    "        parsed\n",
    "            .drop(\"line\")\n",
    "            .withColumn(\"source\",F.lit(\"NMDB\"))\n",
    "            .withColumn(\"metric\",F.lit(station))\n",
    "            .withColumn(\"window_duration_s\",F.lit(VAR[\"window_duration_s\"]))\n",
    "            .withColumn(\"window_start_utc\",  F.to_timestamp(F.col(\"window_start_utc\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "            .withColumn(\"value\",F.col(\"value\").cast(\"double\"))\n",
    "            \n",
    "    )\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dd04ac0-0d52-45bf-b197-fb984158190a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data quality rules\n",
    "\n",
    "This section applies **lightweight, deterministic** cleanup rules before writing to Silver.\n",
    "\n",
    "#### Goals\n",
    "- Remove or neutralize values that are **known-invalid** for a given source/metric.\n",
    "- Keep the dataset **safe to aggregate** (daily means, joins across sources, anomaly detection).\n",
    "- Preserve raw coverage while preventing invalid points from contaminating downstream results.\n",
    "\n",
    "#### Approach\n",
    "- Rules are implemented as **small Spark transforms** that only modify `value` (typically by setting invalid values to `null`).\n",
    "- Rules are **source/metric-specific** (e.g., NMDB vs GFZ Hp30).\n",
    "- Duplicate windows are handled explicitly to ensure **one value per** `(window_start_utc, source, metric)`.\n",
    "\n",
    "#### Output guarantee\n",
    "After this section, the dataset should satisfy:\n",
    "- stable schema: `window_start_utc, source, metric, value, window_duration_s`\n",
    "- one row per window/source/metric (after duplicate collapse)\n",
    "- invalid values are nulled rather than silently kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdb8819d-d694-4721-8815-9b8d3b5c9221",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def dq_null_negative_nmdb(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Null-out invalid negative NMDB values.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Canonical Silver-shaped DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Same schema as input with negative NMDB values replaced by null.\n",
    "    \"\"\"\n",
    "    return df.withColumn(\n",
    "        \"value\",\n",
    "        F.when((F.col(\"source\") == \"NMDB\") & (F.col(\"value\") < 0), F.lit(None)).otherwise(F.col(\"value\")),\n",
    "    )\n",
    "\n",
    "def dq_null_out_of_range_hp30(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Null-out GFZ Hp30 values outside the configured allowed range.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Canonical Silver-shaped DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Same schema as input with out-of-range Hp30 values replaced by null.\n",
    "    \"\"\"\n",
    "    min_val, max_val = VAR[\"gfz_hp30_range\"]\n",
    "    cond = (\n",
    "        (F.col(\"source\") == \"GFZ\")\n",
    "        & (F.col(\"metric\") == \"Hp30\")\n",
    "        & ((F.col(\"value\") < min_val) | (F.col(\"value\") > max_val))\n",
    "    )\n",
    "    return df.withColumn(\"value\", F.when(cond, F.lit(None)).otherwise(F.col(\"value\")))\n",
    "\n",
    "def dq_collapse_duplicates(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Collapse duplicate keys to a single row per (window_start_utc, source, metric).\n",
    "\n",
    "    This pipeline treats duplicate rows for the same (window_start_utc, source, metric)\n",
    "    as equivalent because they represent the same aggregation window. When duplicates\n",
    "    exist, retaining *any* non-null value is acceptable.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Canonical Silver-shaped DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Canonical DataFrame with duplicate keys collapsed to one value.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        df.groupBy(\"window_start_utc\", \"source\", \"metric\")\n",
    "          .agg(\n",
    "              F.first(\"value\", ignorenulls=True).alias(\"value\"),\n",
    "              F.first(\"window_duration_s\", ignorenulls=True).alias(\"window_duration_s\"),\n",
    "          )\n",
    "    )\n",
    "\n",
    "def dq_filter_existing_keys(df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Drop rows whose keys already exist in the Silver Delta table.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): Canonical Silver-shaped DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Input rows excluding keys already present in VAR[\"silver_output\"].\n",
    "    \"\"\"\n",
    "    silver_path = VAR[\"silver_output\"]\n",
    "    try:\n",
    "        existing = (\n",
    "            spark.read.format(\"delta\")\n",
    "                 .load(silver_path)\n",
    "                 .select(\"window_start_utc\", \"source\", \"metric\")\n",
    "                 .dropDuplicates()\n",
    "        )\n",
    "    except Exception:\n",
    "        return df\n",
    "\n",
    "    return df.join(existing, on=[\"window_start_utc\", \"source\", \"metric\"], how=\"left_anti\")\n",
    "\n",
    "VAR[\"data_quality_funcs\"] = [\n",
    "    dq_collapse_duplicates,\n",
    "    dq_null_negative_nmdb,\n",
    "    dq_null_out_of_range_hp30,\n",
    "    dq_filter_existing_keys,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "259b7672-5eba-41a6-8484-46e31f5e1155",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Show Time\n",
    "\n",
    "This is the **main execution** section of the notebook.\n",
    "\n",
    "It runs the pipeline end-to-end:\n",
    "\n",
    "1. Load `VAR` configuration (`global_variables()`)\n",
    "2. Ensure the Silver Delta destination exists (`ensure_silver_delta()`)\n",
    "3. Read Bronze inputs (GFZ / NMDB)\n",
    "4. Standardize into the common Silver schema:\n",
    "   `window_start_utc, source, metric, value, window_duration_s`\n",
    "5. Apply data-quality rules and de-duplication\n",
    "6. Write/append the final result to `VAR[\"silver_output\"]`\n",
    "\n",
    "The cells below are the “orchestration layer”: they call the functions defined above in the intended order and show the resulting DataFrames / write outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90a03ec3-99a6-40e3-bc43-4112bd9ba24c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_files = VAR[\"bronze_files\"]\n",
    "silver_path = VAR[\"silver_output\"]\n",
    "\n",
    "# Ensure destination exists as a Delta table (empty if new)\n",
    "ensure_silver_delta()\n",
    "\n",
    "dfs = []\n",
    "for bcfpath in bronze_files.values():\n",
    "    if not bcfpath or not str(bcfpath).strip():\n",
    "        continue\n",
    "    \n",
    "    filename = bcfpath.split(\"/\")[-1]\n",
    "    task = filename.split(\"_\")[0].lower()\n",
    "    match task:\n",
    "        case \"gfz\":\n",
    "            df = readbc_gfz(bcfpath)\n",
    "        case \"nmdb\":\n",
    "            df = readbc_nmdb(bcfpath)\n",
    "        case _:\n",
    "            raise ValueError(f\"Unknown task prefix for '{task}'. Expected 'gfz_*' or 'nmdb_*'.\")\n",
    "\n",
    "    # Keep a consistent final schema/order\n",
    "    df = df.select(\"window_start_utc\", \"source\", \"metric\", \"value\", \"window_duration_s\")\n",
    "\n",
    "    dfs.append(df)\n",
    "\n",
    "if dfs:\n",
    "    outdf = dfs[0]\n",
    "    for d in dfs[1:]:\n",
    "        outdf = outdf.unionByName(d, allowMissingColumns=True)\n",
    "\n",
    "    for dqfunc in VAR[\"data_quality_funcs\"]:\n",
    "        outdf = dqfunc(outdf)\n",
    "\n",
    "    (\n",
    "        outdf.write.format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .save(silver_path)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6546081a-c104-40d2-8d72-5bee13794fb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Content Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8df8dda-9ca7-41a5-8240-4c41a122d8c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.read.format(\"delta\").load(VAR[\"silver_output\"]).limit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2204616-a147-4a1f-ab9b-4fef597d7906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Saving Information for Next Notebooks\n",
    "\n",
    "This section documents the process and results of saving data to the  Gold layer.  \n",
    "It includes details about the written paths and any updates performed during the current run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6373e7d2-8e6b-4b75-985c-468379ae22cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defing output\n",
    "output_data = {\n",
    "    \"container\": VAR[\"container\"],\n",
    "    \"output_silver\": VAR[\"silver_output\"],\n",
    "}\n",
    "\n",
    "# Return the dictionary\n",
    "dbutils.jobs.taskValues.set(key=\"silver_output\", value=output_data)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1_silver",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "pylinjh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
