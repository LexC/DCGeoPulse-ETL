{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "498639cc-2aec-41b2-a2a1-182432b723b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Bronze ingestion: Space Weather (GFZ + NMDB)\n",
    "\n",
    "This notebook ingests:\n",
    "- **GFZ Potsdam** geomagnetic index time series (e.g., Hp30)\n",
    "- **NMDB NEST** neutron monitor station data (ASCII)\n",
    "\n",
    "Data is written to the **Bronze** layer as **single files** (not Spark partition folders).\n",
    "\n",
    "**Environment:** Databricks (Spark + `dbutils` required)  \n",
    "**Timestamps:** `YYYY-MM-DD` or `YYYY-MM-DDTHH:MM:SSZ` (UTC)  \n",
    "**Workflow:** if the destination folder is empty → bootstrap (2025-01-01 → yesterday UTC); else → incremental update based on the latest saved file.\n",
    "\n",
    "**Filename convention:** includes `start-YYYY-MM-DD` and `end-YYYY-MM-DD` so `update_data()` can infer the next missing range.\n",
    "\n",
    "## How to run\n",
    "\n",
    "1. Attach this notebook to a Databricks cluster with Spark + `dbutils`.\n",
    "2. Edit `global_variables()` for your environment:\n",
    "   - Update the `container`/paths if your storage account differs.\n",
    "   - Update `tasks` to add or change sources:\n",
    "     - Format is `\"<source>_<id>\"`, e.g., `gfz_Hp30` or `nmdb_OULU`.\n",
    "     - Use more GFZ indices by adding more `gfz_<index>` entries.\n",
    "     - Use more NMDB stations by adding more `nmdb_<station>` entries.\n",
    "3. Run all cells\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17fcc544-90fb-401c-af0c-200145e3f8c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Libraries\n",
    "\n",
    "Standard library + Databricks/Spark utilities. No third-party Python packages required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1663e3bf-abbd-4a8b-9756-0f5a0bf049f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "from typing import Tuple\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "from urllib.parse import urlencode\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import URLError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5aaafb4-515d-4dd7-9e0c-636a349cb063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Start Functions\n",
    "\n",
    "### Global configuration\n",
    "\n",
    "All configuration lives in `VAR`, including:\n",
    "- output directories per task\n",
    "- allowed NMDB stations\n",
    "- mapping of task → import function (`save_gfz` / `save_nmdb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd9b44a9-e1a7-4659-962c-06bfd4f91f98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def global_variables():\n",
    "    \"\"\"\n",
    "    This function defines and returns a dictionary of key variables used in the script.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing key variables for configuration and use across the script.\n",
    "    \"\"\"\n",
    "\n",
    "    tiers = [\"bronze\", \"silver\", \"gold\"]\n",
    "    container = {tier: f\"abfss://{tier}@alexccrv0dcn.dfs.core.windows.net\" for tier in tiers}\n",
    "\n",
    "    tasks = [\"gfz_Hp30\",\"nmdb_JUNG1\",\"nmdb_OULU\",\"nmdb_ROME\"]\n",
    "    outputdirs = {task: \"/\".join([container[\"bronze\"],task]) for task in tasks}\n",
    "\n",
    "    VAR = {\n",
    "        \"container\": container,\n",
    "        \"outputdirs\": outputdirs,\n",
    "        \"GFZ_BASE_URL\": \"https://kp.gfz-potsdam.de/app/json/\",\n",
    "        \"NMDB_BASE_URL\": \"https://www.nmdb.eu/nest/draw_graph.php\",\n",
    "    }\n",
    "    return VAR\n",
    "VAR = global_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "309c7b83-9182-4bdd-96bf-872a74f7e554",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Helpers\n",
    "\n",
    "Reusable utilities:\n",
    "- `parse_utc(...)`: parse date-only or ISO-Z timestamps into UTC-aware datetimes\n",
    "- `normalize_utc(...)`: normalize inputs to `YYYY-MM-DDTHH:MM:SSZ`\n",
    "\n",
    "Date-only inputs expand to day boundaries:\n",
    "- start → `00:00:00Z`\n",
    "- end → `23:59:59Z` (or `end_seconds` if overridden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "110c05df-1c8b-45da-bea8-3a29b4e44c39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parse_utc(ts: str, is_start: bool=True, *, end_seconds: int = 59) -> datetime:\n",
    "    \"\"\"\n",
    "    Parse a UTC timestamp string into a timezone-aware `datetime` (UTC).\n",
    "\n",
    "    Accepts either a date-only string (`YYYY-MM-DD`) or a Zulu timestamp\n",
    "    (`YYYY-MM-DDTHH:MM:SSZ`). For date-only inputs, expands the time to a window\n",
    "    boundary determined by `is_start`.\n",
    "\n",
    "    Args:\n",
    "        ts (str): Timestamp in `YYYY-MM-DD` or `YYYY-MM-DDTHH:MM:SSZ` format (UTC).\n",
    "        is_start (bool): If True and `ts` is date-only, use `00:00:00Z`. If False and\n",
    "            `ts` is date-only, use `23:59:{end_seconds:02d}Z`.\n",
    "        end_seconds (int): Second value (0-59) used when `is_start` is False and `ts`\n",
    "            is date-only.\n",
    "\n",
    "    Returns:\n",
    "        datetime: Timezone-aware `datetime` in UTC.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `ts` does not match an accepted format, contains invalid date/time\n",
    "            components, or if `end_seconds` is outside 0-59.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- SETUP AND VALIDATION ---\n",
    "    ts = ts.strip()\n",
    "\n",
    "    if not (0 <= end_seconds <= 59):\n",
    "        raise ValueError(f\"end_seconds must be in 0..59. Got: {end_seconds!r}\")\n",
    "\n",
    "    # --- LOGIC ---\n",
    "    if len(ts) == 10:\n",
    "        base_dt = datetime.strptime(ts, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "        if is_start:\n",
    "            return base_dt.replace(hour=0, minute=0, second=0)\n",
    "        return base_dt.replace(hour=23, minute=59, second=end_seconds)\n",
    "\n",
    "    if ts.endswith(\"Z\"):\n",
    "        return datetime.strptime(ts, \"%Y-%m-%dT%H:%M:%SZ\").replace(tzinfo=timezone.utc)\n",
    "\n",
    "    # --- RETURN ---\n",
    "    raise ValueError(\n",
    "        \"Invalid timestamp format. Use 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SSZ'. \"\n",
    "        f\"Got: {ts!r}\"\n",
    "    )\n",
    "\n",
    "def normalize_utc(ts: str, is_start: bool = True, *, end_seconds: int = 59) -> str:\n",
    "    \"\"\"\n",
    "    Normalize a UTC timestamp string to ISO 8601 Zulu format.\n",
    "\n",
    "    Args:\n",
    "        ts (str): Timestamp in 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SSZ' (UTC).\n",
    "        is_start (bool): Whether `ts` represents a start boundary (00:00:00Z) or\n",
    "            an end boundary (23:59:end_secondsZ) when `ts` is date-only.\n",
    "        end_seconds (int): Second value used for the end boundary when `ts` is\n",
    "            date-only.\n",
    "\n",
    "    Returns:\n",
    "        str: Normalized UTC Zulu timestamp 'YYYY-MM-DDTHH:MM:SSZ'.\n",
    "    \"\"\"\n",
    "    dt = parse_utc(ts, is_start=is_start, end_seconds=end_seconds)\n",
    "    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "def urlopen_with_retries(\n",
    "    url: str,\n",
    "    retries: int = 10,\n",
    "    base_sleep: float = 0.5,\n",
    "    timeout: int = 30,\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Open a URL and return its body as UTF-8 text, retrying on URLError.\n",
    "\n",
    "    This function can mitigate intermittent network issues (including temporary DNS resolution\n",
    "    failures) by retrying with exponential backoff and jitter. It does not fix underlying\n",
    "    network/DNS configuration; it only reduces the impact of intermittent failures.\n",
    "\n",
    "    Between failed attempts, it sleeps for an exponentially increasing delay computed as\n",
    "    base_sleep * (2**attempt), multiplied by a random jitter factor in [0.5, 1.5] to reduce\n",
    "    synchronized retries across concurrent workers.\n",
    "\n",
    "    Jitter rule:\n",
    "        Each retry sleep is the exponential backoff value multiplied by a random factor in [0.5, 1.5].\n",
    "        Example: base_sleep=0.5 -> first retry sleeps in [0.25, 0.75].\n",
    "\n",
    "    Args:\n",
    "        url: Fully-qualified URL to request.\n",
    "        retries: Maximum number of attempts before failing.\n",
    "        base_sleep: Base sleep time in seconds for exponential backoff.\n",
    "        timeout: Timeout in seconds passed to urlopen().\n",
    "\n",
    "    Returns:\n",
    "        Response body decoded as UTF-8 (invalid bytes replaced).\n",
    "\n",
    "    Raises:\n",
    "        URLError: If all retry attempts fail due to a URL-related error.\n",
    "    \"\"\"\n",
    "    last_err = None\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            with urlopen(url, timeout=timeout) as resp:\n",
    "                return resp.read().decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "        except URLError as e:\n",
    "            last_err = e\n",
    "            if attempt == retries - 1:\n",
    "                break\n",
    "\n",
    "            backoff = base_sleep * (2 ** attempt)\n",
    "            jitter_mult = random.uniform(0.5, 1.5)  # +/- 50%\n",
    "            sleep_s = backoff * jitter_mult\n",
    "            next_attempt = attempt + 2\n",
    "            if attempt == 0: print(url)\n",
    "            print(\n",
    "                f\"[warn] urlopen failed (attempt {attempt + 1}/{retries}): {e}. \"\n",
    "                f\"Retrying in {sleep_s:.2f}s (next attempt {next_attempt}/{retries})...\"\n",
    "            )\n",
    "\n",
    "            time.sleep(sleep_s)\n",
    "\n",
    "    raise last_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ddb078d-4ac5-4473-9907-fe9edde6d130",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## API fetchers\n",
    "\n",
    "Low-level “download only” functions (no filesystem writes):\n",
    "- validate inputs\n",
    "- build request URLs\n",
    "- return raw payloads / parsed tuples\n",
    "\n",
    "GFZ returns parsed arrays (times, values, status).  \n",
    "NMDB returns raw ASCII text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6053de6-cfa1-437c-9377-97b677e29368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### GFZ: geomagnetic disturbance indices\n",
    "\n",
    "Fetches an index time series for a UTC window.\n",
    "\n",
    "Notes:\n",
    "- `index` controls which series is downloaded (e.g., Hp30)\n",
    "- `status` may be supported only for some indices\n",
    "- output is parsed into `(times, values, status)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d19bd06-5320-4dca-b1d7-bcae2e96453b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def getGFZindex(starttime: str, endtime: str,\n",
    "               index: str,status: str = \"all\") -> Tuple[tuple, tuple, tuple]:\n",
    "    \"\"\"\n",
    "    Fetch a GFZ Potsdam geomagnetic index time series for a UTC time window.\n",
    "\n",
    "    Args:\n",
    "        starttime (str): 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SSZ' (UTC).\n",
    "        endtime (str): 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SSZ' (UTC).\n",
    "        index (str): One of: Kp, ap, Ap, Cp, C9, Hp30, Hp60, ap30, ap60, SN, Fobs, Fadj.\n",
    "        status (str): 'all' or 'def' (where supported). Default: 'all'.\n",
    "\n",
    "    Returns:\n",
    "        tuple[tuple[str, ...], tuple[object, ...], tuple[str, ...]]: (times, values, status).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Invalid inputs or endtime < starttime.\n",
    "        RuntimeError: Request/parse failure.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- SETUP AND VALIDATION ---\n",
    "    allowed_indices = {\n",
    "        \"Kp\", \"ap\", \"Ap\", \"Cp\", \"C9\", \"Hp30\", \"Hp60\", \"ap30\", \"ap60\", \"SN\", \"Fobs\", \"Fadj\"\n",
    "    }\n",
    "    allowed_status = {\"all\", \"def\"}\n",
    "\n",
    "    if index not in allowed_indices:\n",
    "        raise ValueError(\n",
    "            \"Wrong index parameter. Allowed: \"\n",
    "            \"'Kp','ap','Ap','Cp','C9','Hp30','Hp60','ap30','ap60','SN','Fobs','Fadj'. \"\n",
    "            f\"Got: {index!r}\"\n",
    "        )\n",
    "\n",
    "    if status not in allowed_status:\n",
    "        raise ValueError(\"Wrong status parameter. Allowed: 'all', 'def'. Got: {!r}\".format(status))\n",
    "\n",
    "    # Start: 00:00:00Z if date-only; End: 23:59:59Z if date-only (full-day coverage)\n",
    "    d1 = parse_utc(starttime, is_start=True)\n",
    "    d2 = parse_utc(endtime, is_start=False, end_seconds=59)\n",
    "\n",
    "    if d1 > d2:\n",
    "        raise ValueError(f\"Start time must be <= end time. Got: {d1.isoformat()} > {d2.isoformat()}\")\n",
    "\n",
    "    # --- LOGIC ---\n",
    "    time_string = (\n",
    "        f\"start={d1.strftime('%Y-%m-%dT%H:%M:%SZ')}\"\n",
    "        f\"&end={d2.strftime('%Y-%m-%dT%H:%M:%SZ')}\"\n",
    "    )\n",
    "    url = f\"{VAR['GFZ_BASE_URL']}?{time_string}&index={index}\"\n",
    "\n",
    "    if status == \"def\":\n",
    "        url += \"&status=def\"\n",
    "\n",
    "    try:\n",
    "        payload = urlopen_with_retries(url)\n",
    "        data = json.loads(payload)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to fetch/parse GFZ response. URL={url!r}. Error: {e}\") from e\n",
    "\n",
    "    datetime_values = tuple(data.get(\"datetime\", ()))\n",
    "    index_values = tuple(data.get(index, ()))\n",
    "\n",
    "    status_values = tuple(data.get(\"status\", ()))\n",
    "    \n",
    "    # --- RETURN ---\n",
    "    return datetime_values, index_values, status_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "066bba71-4ddc-4729-b66c-7af8d5a2038f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### NMDB: NEST neutron monitor stations\n",
    "\n",
    "Downloads station data for a UTC window as raw ASCII.\n",
    "\n",
    "Notes:\n",
    "- `station` must be in the allowlist (prevents typos and unexpected calls)\n",
    "- payload is written **as-is** to preserve the source format for Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "055a1e8e-dc96-481f-98d8-060ce84e7b53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def getNMDBnest(starttime: str, endtime: str, station:str):\n",
    "    \"\"\"\n",
    "    Fetch NMDB NEST neutron monitor data (ASCII) for one station over a UTC window.\n",
    "\n",
    "    Args:\n",
    "        starttime (str): Start time in 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SSZ' (UTC).\n",
    "        endtime (str): End time in 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SSZ' (UTC).\n",
    "        station (str): NMDB station code (must be in the allowed station list).\n",
    "\n",
    "    Returns:\n",
    "        str: Raw ASCII response text returned by the NMDB NEST endpoint.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If timestamps are invalid, endtime < starttime, or station is not\n",
    "            allowed.\n",
    "        urllib.error.URLError: If the request fails (network/DNS/timeout).\n",
    "        urllib.error.HTTPError: If NMDB returns a non-2xx HTTP status.\n",
    "    \"\"\"\n",
    "    # --- SETUP AND VALIDATION ---\n",
    "    start_dt = parse_utc(starttime, is_start=True)\n",
    "    end_dt = parse_utc(endtime, is_start=False)\n",
    "    if end_dt < start_dt:\n",
    "        raise ValueError(\"endtime must be >= starttime.\")\n",
    "\n",
    "    NMDB_ALLOWED_STATIONS = [\n",
    "        \"AATA\",\"AATB\",\"AHMD\",\"APTY\",\"ARNM\",\"ATHN\",\"BKSN\",\"BUDA\",\"CALG\",\n",
    "        \"CALM\",\"CHAC\",\"CLMX\",\"DJON\",\"DOMB\",\"DOMC\",\"DRBS\",\"DRHM\",\"ESOI\",\n",
    "        \"FSMT\",\"HRMS\",\"HUAN\",\"ICRB\",\"ICRO\",\"INVK\",\"IRK2\",\"IRK3\",\"IRKT\",\n",
    "        \"JBGO\",\"JUNG\",\"JUNG1\",\"KERG\",\"KGSN\",\"KIEL\",\"KIEL2\",\"LMKS\",\"MCMU\",\n",
    "        \"MCRL\",\"MGDN\",\"MOSC\",\"MRNY\",\"MWSB\",\"MWSN\",\"MXCO\",\"NAIN\",\"NANM\",\n",
    "        \"NEU3\",\"NEWK\",\"NRLK\",\"NVBK\",\"OULU\",\"PSNM\",\"PTFM\",\"PWNK\",\"ROME\",\n",
    "        \"SANB\",\"SNAE\",\"SOPB\",\"SOPO\",\"TERA\",\"THUL\",\"TSMB\",\"TXBY\",\"UFSZ\",\n",
    "        \"YKTK\",\"ZUGS\",\n",
    "    ]\n",
    "    \n",
    "    if station not in NMDB_ALLOWED_STATIONS:\n",
    "        raise ValueError(\n",
    "            f\"NMDB station not allowed: {station}. \"\n",
    "            f\"Allowed: {NMDB_ALLOWED_STATIONS}\"\n",
    "        )\n",
    "    \n",
    "    params = [\n",
    "        (\"formchk\", \"1\"),\n",
    "        (\"stations[]\", station),\n",
    "        (\"tabchoice\", \"revori\"),\n",
    "        (\"dtype\", \"corr_for_pressure\"),\n",
    "        (\"tresolution\", \"30\"),\n",
    "        (\"date_choice\", \"bydate\"),\n",
    "\n",
    "        (\"start_year\", f\"{start_dt.year:04d}\"),\n",
    "        (\"start_month\", f\"{start_dt.month:02d}\"),\n",
    "        (\"start_day\", f\"{start_dt.day:02d}\"),\n",
    "        (\"start_hour\", f\"{start_dt.hour:d}\"),\n",
    "        (\"start_min\", f\"{start_dt.minute:d}\"),\n",
    "\n",
    "        (\"end_year\", f\"{end_dt.year:04d}\"),\n",
    "        (\"end_month\", f\"{end_dt.month:02d}\"),\n",
    "        (\"end_day\", f\"{end_dt.day:02d}\"),\n",
    "        (\"end_hour\", f\"{end_dt.hour:d}\"),\n",
    "        (\"end_min\", f\"{end_dt.minute:d}\"),\n",
    "\n",
    "        (\"output\", \"ascii\"),\n",
    "        (\"yunits\", \"0\"),\n",
    "        (\"anomalous\", \"1\"),\n",
    "        (\"display_null\", \"1\"),\n",
    "    ]\n",
    "\n",
    "    # --- LOGIC ---\n",
    "    url = VAR[\"NMDB_BASE_URL\"] + \"?\" + urlencode(params, doseq=True)\n",
    "    \n",
    "    text = urlopen_with_retries(url)\n",
    "\n",
    "    # --- RETURN ---\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19543c9a-e05e-48e3-a873-398da5a2ce2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Saving data (single-file Bronze writes)\n",
    "\n",
    "This section contains the persistence utilities and “fetch + persist” entrypoints:\n",
    "\n",
    "- `write_single_file(...)`: forces **one output file** (Spark `coalesce(1)` → rename `part-*`)\n",
    "- `save_gfz(...)`: fetches via `getGFZindex(...)`, serializes to CSV, writes a single file\n",
    "- `save_nmdb(...)`: fetches via `getNMDBnest(...)`, writes raw ASCII as a single TXT file\n",
    "\n",
    "All saved filenames embed `start-YYYY-MM-DD` and `end-YYYY-MM-DD` so incremental updates\n",
    "can infer the next missing window from existing Bronze files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3107a92c-63bd-474e-ac96-c60d4559fec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_single_file(dir_path: str, filename: str, content: str) -> str:\n",
    "    \"\"\"\n",
    "    Write a single text file to DBFS/ABFSS via Spark.\n",
    "\n",
    "    Spark text writes produce a directory of part-files, so this helper writes to a\n",
    "    temporary directory with `coalesce(1)`, moves the single `part-*` file to the\n",
    "    requested `{dir_path}/{filename}`, then deletes the temp directory.\n",
    "\n",
    "    Args:\n",
    "        dir_path (str): Destination directory path (DBFS/ABFSS).\n",
    "        filename (str): Target filename to create in `dir_path`.\n",
    "        content (str): Full file contents to write.\n",
    "\n",
    "    Returns:\n",
    "        str: Full path to the written file.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If no `part-*` file is produced in the temporary directory.\n",
    "    \"\"\"\n",
    "\n",
    "    dbutils.fs.mkdirs(dir_path)\n",
    "    target = f\"{dir_path.rstrip('/')}/{filename}\"\n",
    "\n",
    "    # Best-effort remove existing target\n",
    "    try:\n",
    "        dbutils.fs.rm(target)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Unique temp dir (avoid collisions); uses current UTC timestamp\n",
    "    tmp_tag = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    tmp_dir = f\"{dir_path.rstrip('/')}/_tmp_{filename}_{tmp_tag}\"\n",
    "\n",
    "    # Spark write (single partition -> single part-* file)\n",
    "    df = spark.createDataFrame([(content,)], [\"value\"])\n",
    "    df.coalesce(1).write.mode(\"overwrite\").text(tmp_dir)\n",
    "\n",
    "    # Find the part file and move it to the desired filename\n",
    "    part_files = [x.path for x in dbutils.fs.ls(tmp_dir) if x.name.startswith(\"part-\")]\n",
    "    if not part_files:\n",
    "        raise RuntimeError(f\"No part-* file found in {tmp_dir}\")\n",
    "\n",
    "    dbutils.fs.mv(part_files[0], target, True)\n",
    "    dbutils.fs.rm(tmp_dir, True)\n",
    "\n",
    "    return target\n",
    "\n",
    "def save_gfz(outputdir:str, idx:str, startdate: str, enddate: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch a GFZ geomagnetic index for a UTC window and write it as a single CSV file.\n",
    "\n",
    "    Args:\n",
    "        outputdir (str): Destination directory path (DBFS/ABFSS).\n",
    "        idx (str): GFZ index name (e.g., \"Hp30\").\n",
    "        startdate (str): 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SSZ' (UTC).\n",
    "        enddate (str): 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SSZ' (UTC).\n",
    "\n",
    "    Returns:\n",
    "        str: Full path to the written CSV file.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If timestamps are invalid or enddate < startdate.\n",
    "        RuntimeError: If the output file cannot be materialized as a single file.\n",
    "    \"\"\"\n",
    "\n",
    "    start_iso = normalize_utc(startdate, is_start=True)\n",
    "    end_iso   = normalize_utc(enddate, is_start=False)\n",
    "\n",
    "    start_day = start_iso[:10]\n",
    "    end_day   = end_iso[:10]\n",
    "\n",
    "    run_tag = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "\n",
    "    dt_vals, idx_vals, _ = getGFZindex(start_iso, end_iso, idx, \"def\")\n",
    "\n",
    "    csv_lines = [f\"datetime,{idx}\"]\n",
    "    for d, v in zip(dt_vals, idx_vals):\n",
    "        csv_lines.append(f\"{d},{v}\")\n",
    "\n",
    "    filename = f\"gfz_index-{idx}_start-{start_day}_end-{end_day}_tag-{run_tag}.csv\"\n",
    "    return write_single_file(outputdir, filename, \"\\n\".join(csv_lines))\n",
    "\n",
    "\n",
    "def save_nmdb(outputdir:str, station:list,startdate: str, enddate: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch NMDB NEST data for one station over a UTC window and write it as a single TXT file.\n",
    "\n",
    "    Args:\n",
    "        outputdir (str): Destination directory path (DBFS/ABFSS).\n",
    "        station (str): NMDB station code (must be allowed by `getNMDBnest`).\n",
    "        startdate (str): 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SSZ' (UTC).\n",
    "        enddate (str): 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SSZ' (UTC).\n",
    "\n",
    "    Returns:\n",
    "        str: Full path to the written TXT file.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If timestamps are invalid, enddate < startdate, or station is invalid.\n",
    "        urllib.error.URLError: If the request fails (network/DNS/timeout).\n",
    "        urllib.error.HTTPError: If NMDB returns a non-2xx HTTP status.\n",
    "        RuntimeError: If the output file cannot be materialized as a single file.\n",
    "    \"\"\"\n",
    "        \n",
    "    start_iso = normalize_utc(startdate, is_start=True)\n",
    "    end_iso   = normalize_utc(enddate, is_start=False)\n",
    "\n",
    "    start_day = start_iso[:10]\n",
    "    end_day   = end_iso[:10]\n",
    "\n",
    "    run_tag = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "\n",
    "    text = getNMDBnest(start_iso, end_iso,station)\n",
    "\n",
    "    filename = f\"nmdb_stations-{station}_start-{start_day}_end-{end_day}_tag-{run_tag}.txt\"\n",
    "    return write_single_file(outputdir, filename, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba73828-703c-4b99-b9e1-a0cabdacf782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Gathering Functions (registry)\n",
    "\n",
    "To keep orchestration generic, we register the **source-specific save functions** in `VAR['importfuncs']`\n",
    "so they can be called indirectly later (by source key), e.g.:\n",
    "\n",
    "- `save_gfz`  -> GFZ index writer\n",
    "- `save_nmdb` -> NMDB station writer\n",
    "\n",
    "This keeps the \"which source do I call?\" decision out of the orchestration logic and\n",
    "lets the run loop dispatch by `source` + `idx` (where `idx` is the GFZ index or NMDB station code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a67afcd-ec7a-475f-b7f6-78164ac084d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "VAR[\"importfuncs\"] = {\n",
    "    \"gfz\":save_gfz,\n",
    "    \"nmdb\":save_nmdb,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "669b4582-9a00-4f4a-9405-95b5b256ef2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Run: bootstrap or update\n",
    "\n",
    "For each configured task:\n",
    "- if the destination folder is empty → bootstrap\n",
    "- otherwise → incremental update\n",
    "\n",
    "This keeps Bronze append-only and makes reruns idempotent at the “file per window” level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cebe41fd-e191-4485-9af8-a1bd5d850be7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def bootstrap_2025tonow(importfunc,outputdir,idx):\n",
    "    \"\"\"\n",
    "    Backfill a full 2025-to-yesterday (UTC) window into Bronze for one source/index.\n",
    "\n",
    "    Computes the window `2025-01-01T00:00:00Z` to end-of-yesterday (UTC), normalizes\n",
    "    timestamps with `normalize_utc()`, then calls `importfunc(outputdir, idx, start_iso, end_iso)`.\n",
    "\n",
    "    Args:\n",
    "        importfunc (Callable): Import/write function (e.g., `save_gfz` or `save_nmdb`).\n",
    "        outputdir (str): Destination directory path (DBFS/ABFSS).\n",
    "        idx (str): Index/station identifier passed through to `importfunc`.\n",
    "\n",
    "    Returns:\n",
    "        Any: Whatever `importfunc` returns (typically the written file path).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the computed end timestamp is before 2025-01-01.\n",
    "    \"\"\"\n",
    "    # --- window: 2025-01-01 .. yesterday (UTC) ---\n",
    "    startdate = \"2025-01-01\"\n",
    "    enddate = (datetime.now(timezone.utc) - timedelta(days=1)).date().isoformat()\n",
    "\n",
    "    # normalize once to validate + for display\n",
    "    start_iso = normalize_utc(startdate, is_start=True)\n",
    "    end_iso = normalize_utc(enddate, is_start=False)\n",
    "\n",
    "    if end_iso < \"2025-01-01T00:00:00Z\":\n",
    "        raise ValueError(f\"Computed end={end_iso} is before 2025-01-01; refusing to bootstrap.\")\n",
    "\n",
    "    return importfunc(outputdir,idx,start_iso, end_iso)\n",
    "\n",
    "def update_data(importfunc,outputdir,idx):\n",
    "    \"\"\"\n",
    "    Incrementally fetch new data since the latest saved file and append it to Bronze.\n",
    "\n",
    "    Scans `outputdir` for existing files, extracts each file's end date from the\n",
    "    filename pattern containing `end-<YYYY-MM-DD>`, then sets the next start date\n",
    "    to (latest_end + 1 day). Uses yesterday (UTC) as the update end date and calls\n",
    "    `importfunc(outputdir, idx, start_iso, end_iso)` when updates are needed.\n",
    "\n",
    "    Args:\n",
    "        importfunc (Callable): Import/write function (e.g., `save_gfz` or `save_nmdb`).\n",
    "        outputdir (str): Destination directory path (DBFS/ABFSS).\n",
    "        idx (str): Index/station identifier passed through to `importfunc`.\n",
    "\n",
    "    Returns:\n",
    "        Any | None: `importfunc` result (typically a written file path), or None if\n",
    "            no update is needed.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If timestamps or filename-derived dates cannot be parsed.\n",
    "        Exception: If listing `outputdir` or `importfunc` execution fails.\n",
    "    \"\"\"\n",
    "    \n",
    "    startdate = parse_utc(\"2025-01-01\",is_start=True)\n",
    "    for file in dbutils.fs.ls(outputdir):\n",
    "        file_enddate = parse_utc(file.name.split(\"end-\")[1].split(\"_\")[0],is_start=False)\n",
    "        file_nextday = parse_utc(\n",
    "            (file_enddate + timedelta(days=1)).date().isoformat(),\n",
    "            is_start=True,\n",
    "        )\n",
    "        \n",
    "        startdate = file_nextday if file_nextday > startdate else startdate\n",
    "    \n",
    "    enddate = parse_utc(\n",
    "        (datetime.now(timezone.utc) - timedelta(days=1)).date().isoformat(),\n",
    "        is_start=False,\n",
    "    )\n",
    "\n",
    "    start_iso = startdate.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    end_iso = enddate.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    \n",
    "    if enddate < startdate:\n",
    "        print (f\"The {outputdir} resource is uptodated to {start_iso}\")\n",
    "        return None\n",
    "    \n",
    "    return importfunc(outputdir,idx,start_iso, end_iso)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a8b300f-c7e9-45ee-9de2-a4f0d8ad08c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Show time\n",
    "\n",
    "The code below is the **main driver** for this notebook.\n",
    "\n",
    "It iterates over `VAR[\"outputdirs\"]`, derives `source` + `idx` from the task name,\n",
    "checks whether the destination folder is empty, and then dispatches to:\n",
    "- `bootstrap_2025tonow(...)` for first-time backfills (2025 → yesterday UTC)\n",
    "- `update_data(...)` for incremental ingestion since the latest saved end date\n",
    "\n",
    "At the end it prints the written paths (bootstrap and/or update) per task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fdf3620-7b66-4b51-88ca-861ee29a5aae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bootstrap_message = {}\n",
    "updated_message = {}\n",
    "content_examples = {}\n",
    "examples_limit = 2\n",
    "examples_count = 0\n",
    "for task,outputdir in VAR[\"outputdirs\"].items():\n",
    "\n",
    "    source,idx = task.split(\"_\")\n",
    "\n",
    "    try: flag_empty = len(dbutils.fs.ls(outputdir)) == 0\n",
    "    except Exception: flag_empty = True  # missing/unlistable -> treat as empty\n",
    "\n",
    "    if flag_empty:\n",
    "        data = bootstrap_2025tonow(VAR[\"importfuncs\"][source],outputdir,idx)\n",
    "        if data: bootstrap_message[task] = data\n",
    "    else:\n",
    "        data = update_data(VAR[\"importfuncs\"][source],outputdir,idx)\n",
    "        if data: updated_message[task] = data\n",
    "    \n",
    "    if examples_count < examples_limit:\n",
    "        examples_count += 1\n",
    "        if source == \"gfz\":\n",
    "            content_examples[\"gfz\"] = data\n",
    "        elif source == \"nmdb\":\n",
    "            content_examples[\"nmdb\"] = data\n",
    "\n",
    "if bootstrap_message:\n",
    "    print(\"Bootstrap complete. Written paths:\")\n",
    "    for k, v in bootstrap_message.items():\n",
    "        print(f\"- {k}: {v}\")\n",
    "\n",
    "if updated_message:\n",
    "    print(\"\\nUpdate complete. Written paths:\")\n",
    "    for k, v in updated_message.items():\n",
    "        print(f\"- {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "273ec499-f12e-4a74-9d1c-28ebb35152ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Content Display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dab812d5-7b91-40e6-988a-90df6b2c4a61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display GFZ example if available\n",
    "if \"gfz\" in content_examples and content_examples[\"gfz\"]:\n",
    "    gfz_path = content_examples[\"gfz\"]\n",
    "    gfz_df = spark.read.csv(gfz_path, header=True)\n",
    "    print(f\"GFZ Example File: {gfz_path}\")\n",
    "    display(gfz_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "089b721b-bc80-4e25-a1fd-ea7ee8d1b573",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display NMDB example if available\n",
    "if \"nmdb\" in content_examples and content_examples[\"nmdb\"]:\n",
    "    nmdb_path = content_examples[\"nmdb\"]\n",
    "    nmdb_text = spark.read.text(nmdb_path)\n",
    "    print(f\"NMDB Example File: {nmdb_path}\")\n",
    "    display(nmdb_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae9ad68f-e998-4023-b9f1-799c91038c23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Saving Information for Next Notebooks\n",
    "\n",
    "This section documents the process and results of saving data to the Silver and Gold layer.  \n",
    "It includes details about the written paths and any updates performed during the current run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3e4606c-97ba-4db4-a246-beb7ffe55b7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Defing output\n",
    "output_bronze = {k:v for k, v in bootstrap_message.items()}\n",
    "if updated_message:\n",
    "    for k, v in updated_message.items():\n",
    "        output_bronze[k] = v\n",
    "\n",
    "output_data = {\n",
    "    \"container\": VAR[\"container\"],\n",
    "    \"output_bronze\": output_bronze,\n",
    "}\n",
    "\n",
    "# Return the dictionary\n",
    "dbutils.jobs.taskValues.set(key=\"bronze_output\", value=output_data)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "0_bronze",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "pylinjh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
