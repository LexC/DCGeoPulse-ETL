{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bronze ingestion: Space Weather (GFZ + NMDB)\n",
    "\n",
    "This notebook ingests:\n",
    "- **GFZ Potsdam** geomagnetic index time series (e.g., Hp30)\n",
    "- **NMDB NEST** neutron monitor station data (ASCII)\n",
    "\n",
    "Data is written to the **Bronze** layer as **single files** (not Spark partition folders).\n",
    "\n",
    "**Environment:** Databricks (Spark + `dbutils` required)  \n",
    "**Timestamps:** `YYYY-MM-DD` or `YYYY-MM-DDTHH:MM:SSZ` (UTC)  \n",
    "**Workflow:** if the destination folder is empty → bootstrap (2025-01-01 → yesterday UTC); else → incremental update based on the latest saved file.\n",
    "\n",
    "**Filename convention:** includes `start-YYYY-MM-DD` and `end-YYYY-MM-DD` so `update_data()` can infer the next missing range.\n",
    "\n",
    "## How to run\n",
    "\n",
    "1. Attach this notebook to a Databricks cluster with Spark + `dbutils`.\n",
    "2. Edit `global_variables()` for your environment:\n",
    "   - Update the `container`/paths if your storage account differs.\n",
    "   - Update `tasks` to add or change sources:\n",
    "     - Format is `\"<source>_<id>\"`, e.g., `gfz_Hp30` or `nmdb_OULU`.\n",
    "     - Use more GFZ indices by adding more `gfz_<index>` entries.\n",
    "     - Use more NMDB stations by adding more `nmdb_<station>` entries.\n",
    "3. Run all cells\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17fcc544-90fb-401c-af0c-200145e3f8c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Libraries\n",
    "\n",
    "Standard library + Databricks/Spark utilities. No third-party Python packages required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1663e3bf-abbd-4a8b-9756-0f5a0bf049f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from typing import Tuple\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "from urllib.parse import urlencode\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5aaafb4-515d-4dd7-9e0c-636a349cb063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Start Functions\n",
    "\n",
    "### Global configuration\n",
    "\n",
    "All configuration lives in `VAR`, including:\n",
    "- output directories per task\n",
    "- allowed NMDB stations\n",
    "- mapping of task → import function (`save_gfz` / `save_nmdb`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fd9b44a9-e1a7-4659-962c-06bfd4f91f98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def global_variables():\n",
    "    \"\"\"\n",
    "    This function defines and returns a dictionary of key variables used in the script.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing key variables for configuration and use across the script.\n",
    "    \"\"\"\n",
    "\n",
    "    tiers = [\"bronze\", \"silver\", \"gold\"]\n",
    "    container = {tier: f\"abfss://{tier}@alexccrv0dcn.dfs.core.windows.net\" for tier in tiers}\n",
    "\n",
    "    tasks = [\"gfz_Hp30\",\"nmdb_JUNG1\",\"nmdb_OULU\",\"nmdb_ROME\"]\n",
    "    outputdirs = {task: \"/\".join([container[\"bronze\"],task]) for task in tasks}\n",
    "\n",
    "    VAR = {\n",
    "        \"container\": container,\n",
    "        \"outputdirs\": outputdirs,\n",
    "        \"GFZ_BASE_URL\": \"https://kp.gfz-potsdam.de/app/json/\",\n",
    "        \"NMDB_BASE_URL\": \"https://www.nmdb.eu/nest/draw_graph.php\",\n",
    "    }\n",
    "    return VAR\n",
    "VAR = global_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "309c7b83-9182-4bdd-96bf-872a74f7e554",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Helpers\n",
    "\n",
    "Reusable utilities:\n",
    "- `parse_utc(...)`: parse date-only or ISO-Z timestamps into UTC-aware datetimes\n",
    "- `normalize_utc(...)`: normalize inputs to `YYYY-MM-DDTHH:MM:SSZ`\n",
    "\n",
    "Date-only inputs expand to day boundaries:\n",
    "- start → `00:00:00Z`\n",
    "- end → `23:59:59Z` (or `end_seconds` if overridden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_utc(ts: str, is_start: bool=True, *, end_seconds: int = 59) -> datetime:\n",
    "    \"\"\"\n",
    "    Parse a UTC timestamp string into a timezone-aware `datetime` (UTC).\n",
    "\n",
    "    Accepts either a date-only string (`YYYY-MM-DD`) or a Zulu timestamp\n",
    "    (`YYYY-MM-DDTHH:MM:SSZ`). For date-only inputs, expands the time to a window\n",
    "    boundary determined by `is_start`.\n",
    "\n",
    "    Args:\n",
    "        ts (str): Timestamp in `YYYY-MM-DD` or `YYYY-MM-DDTHH:MM:SSZ` format (UTC).\n",
    "        is_start (bool): If True and `ts` is date-only, use `00:00:00Z`. If False and\n",
    "            `ts` is date-only, use `23:59:{end_seconds:02d}Z`.\n",
    "        end_seconds (int): Second value (0-59) used when `is_start` is False and `ts`\n",
    "            is date-only.\n",
    "\n",
    "    Returns:\n",
    "        datetime: Timezone-aware `datetime` in UTC.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If `ts` does not match an accepted format, contains invalid date/time\n",
    "            components, or if `end_seconds` is outside 0-59.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- SETUP AND VALIDATION ---\n",
    "    ts = ts.strip()\n",
    "\n",
    "    if not (0 <= end_seconds <= 59):\n",
    "        raise ValueError(f\"end_seconds must be in 0..59. Got: {end_seconds!r}\")\n",
    "\n",
    "    # --- LOGIC ---\n",
    "    if len(ts) == 10:\n",
    "        base_dt = datetime.strptime(ts, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n",
    "        if is_start:\n",
    "            return base_dt.replace(hour=0, minute=0, second=0)\n",
    "        return base_dt.replace(hour=23, minute=59, second=end_seconds)\n",
    "\n",
    "    if ts.endswith(\"Z\"):\n",
    "        return datetime.strptime(ts, \"%Y-%m-%dT%H:%M:%SZ\").replace(tzinfo=timezone.utc)\n",
    "\n",
    "    # --- RETURN ---\n",
    "    raise ValueError(\n",
    "        \"Invalid timestamp format. Use 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SSZ'. \"\n",
    "        f\"Got: {ts!r}\"\n",
    "    )\n",
    "\n",
    "def normalize_utc(ts: str, is_start: bool = True, *, end_seconds: int = 59) -> str:\n",
    "    \"\"\"\n",
    "    Normalize a UTC timestamp string to ISO 8601 Zulu format.\n",
    "\n",
    "    Args:\n",
    "        ts (str): Timestamp in 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SSZ' (UTC).\n",
    "        is_start (bool): Whether `ts` represents a start boundary (00:00:00Z) or\n",
    "            an end boundary (23:59:end_secondsZ) when `ts` is date-only.\n",
    "        end_seconds (int): Second value used for the end boundary when `ts` is\n",
    "            date-only.\n",
    "\n",
    "    Returns:\n",
    "        str: Normalized UTC Zulu timestamp 'YYYY-MM-DDTHH:MM:SSZ'.\n",
    "    \"\"\"\n",
    "    dt = parse_utc(ts, is_start=is_start, end_seconds=end_seconds)\n",
    "    return dt.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API fetchers\n",
    "\n",
    "Low-level “download only” functions (no filesystem writes):\n",
    "- validate inputs\n",
    "- build request URLs\n",
    "- return raw payloads / parsed tuples\n",
    "\n",
    "GFZ returns parsed arrays (times, values, status).  \n",
    "NMDB returns raw ASCII text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GFZ: geomagnetic disturbance indices\n",
    "\n",
    "Fetches an index time series for a UTC window.\n",
    "\n",
    "Notes:\n",
    "- `index` controls which series is downloaded (e.g., Hp30)\n",
    "- `status` may be supported only for some indices\n",
    "- output is parsed into `(times, values, status)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGFZindex(starttime: str, endtime: str,\n",
    "               index: str,status: str = \"all\") -> Tuple[tuple, tuple, tuple]:\n",
    "    \"\"\"\n",
    "    Fetch a GFZ Potsdam geomagnetic index time series for a UTC time window.\n",
    "\n",
    "    Args:\n",
    "        starttime (str): 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SSZ' (UTC).\n",
    "        endtime (str): 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SSZ' (UTC).\n",
    "        index (str): One of: Kp, ap, Ap, Cp, C9, Hp30, Hp60, ap30, ap60, SN, Fobs, Fadj.\n",
    "        status (str): 'all' or 'def' (where supported). Default: 'all'.\n",
    "\n",
    "    Returns:\n",
    "        tuple[tuple[str, ...], tuple[object, ...], tuple[str, ...]]: (times, values, status).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: Invalid inputs or endtime < starttime.\n",
    "        RuntimeError: Request/parse failure.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- SETUP AND VALIDATION ---\n",
    "    allowed_indices = {\n",
    "        \"Kp\", \"ap\", \"Ap\", \"Cp\", \"C9\", \"Hp30\", \"Hp60\", \"ap30\", \"ap60\", \"SN\", \"Fobs\", \"Fadj\"\n",
    "    }\n",
    "    allowed_status = {\"all\", \"def\"}\n",
    "\n",
    "    if index not in allowed_indices:\n",
    "        raise ValueError(\n",
    "            \"Wrong index parameter. Allowed: \"\n",
    "            \"'Kp','ap','Ap','Cp','C9','Hp30','Hp60','ap30','ap60','SN','Fobs','Fadj'. \"\n",
    "            f\"Got: {index!r}\"\n",
    "        )\n",
    "\n",
    "    if status not in allowed_status:\n",
    "        raise ValueError(\"Wrong status parameter. Allowed: 'all', 'def'. Got: {!r}\".format(status))\n",
    "\n",
    "    # Start: 00:00:00Z if date-only; End: 23:59:59Z if date-only (full-day coverage)\n",
    "    d1 = parse_utc(starttime, is_start=True)\n",
    "    d2 = parse_utc(endtime, is_start=False, end_seconds=59)\n",
    "\n",
    "    if d1 > d2:\n",
    "        raise ValueError(f\"Start time must be <= end time. Got: {d1.isoformat()} > {d2.isoformat()}\")\n",
    "\n",
    "    # --- LOGIC ---\n",
    "    time_string = (\n",
    "        f\"start={d1.strftime('%Y-%m-%dT%H:%M:%SZ')}\"\n",
    "        f\"&end={d2.strftime('%Y-%m-%dT%H:%M:%SZ')}\"\n",
    "    )\n",
    "    url = f\"{VAR['GFZ_BASE_URL']}?{time_string}&index={index}\"\n",
    "\n",
    "    if status == \"def\":\n",
    "        url += \"&status=def\"\n",
    "\n",
    "    try:\n",
    "        with urlopen(url, timeout=30) as resp:\n",
    "            payload = resp.read().decode(\"utf-8\")\n",
    "        data = json.loads(payload)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to fetch/parse GFZ response. URL={url!r}. Error: {e}\") from e\n",
    "\n",
    "    datetime_values = tuple(data.get(\"datetime\", ()))\n",
    "    index_values = tuple(data.get(index, ()))\n",
    "\n",
    "    status_values = tuple(data.get(\"status\", ()))\n",
    "    \n",
    "    # --- RETURN ---\n",
    "    return datetime_values, index_values, status_values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMDB: NEST neutron monitor stations\n",
    "\n",
    "Downloads station data for a UTC window as raw ASCII.\n",
    "\n",
    "Notes:\n",
    "- `station` must be in the allowlist (prevents typos and unexpected calls)\n",
    "- payload is written **as-is** to preserve the source format for Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNMDBnest(starttime: str, endtime: str, station:str):\n",
    "    \"\"\"\n",
    "    Fetch NMDB NEST neutron monitor data (ASCII) for one station over a UTC window.\n",
    "\n",
    "    Args:\n",
    "        starttime (str): Start time in 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SSZ' (UTC).\n",
    "        endtime (str): End time in 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SSZ' (UTC).\n",
    "        station (str): NMDB station code (must be in the allowed station list).\n",
    "\n",
    "    Returns:\n",
    "        str: Raw ASCII response text returned by the NMDB NEST endpoint.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If timestamps are invalid, endtime < starttime, or station is not\n",
    "            allowed.\n",
    "        urllib.error.URLError: If the request fails (network/DNS/timeout).\n",
    "        urllib.error.HTTPError: If NMDB returns a non-2xx HTTP status.\n",
    "    \"\"\"\n",
    "    # --- SETUP AND VALIDATION ---\n",
    "    start_dt = parse_utc(starttime, is_start=True)\n",
    "    end_dt = parse_utc(endtime, is_start=False)\n",
    "    if end_dt < start_dt:\n",
    "        raise ValueError(\"endtime must be >= starttime.\")\n",
    "\n",
    "    NMDB_ALLOWED_STATIONS = [\n",
    "        \"AATA\",\"AATB\",\"AHMD\",\"APTY\",\"ARNM\",\"ATHN\",\"BKSN\",\"BUDA\",\"CALG\",\n",
    "        \"CALM\",\"CHAC\",\"CLMX\",\"DJON\",\"DOMB\",\"DOMC\",\"DRBS\",\"DRHM\",\"ESOI\",\n",
    "        \"FSMT\",\"HRMS\",\"HUAN\",\"ICRB\",\"ICRO\",\"INVK\",\"IRK2\",\"IRK3\",\"IRKT\",\n",
    "        \"JBGO\",\"JUNG\",\"JUNG1\",\"KERG\",\"KGSN\",\"KIEL\",\"KIEL2\",\"LMKS\",\"MCMU\",\n",
    "        \"MCRL\",\"MGDN\",\"MOSC\",\"MRNY\",\"MWSB\",\"MWSN\",\"MXCO\",\"NAIN\",\"NANM\",\n",
    "        \"NEU3\",\"NEWK\",\"NRLK\",\"NVBK\",\"OULU\",\"PSNM\",\"PTFM\",\"PWNK\",\"ROME\",\n",
    "        \"SANB\",\"SNAE\",\"SOPB\",\"SOPO\",\"TERA\",\"THUL\",\"TSMB\",\"TXBY\",\"UFSZ\",\n",
    "        \"YKTK\",\"ZUGS\",\n",
    "    ]\n",
    "    \n",
    "    if station not in NMDB_ALLOWED_STATIONS:\n",
    "        raise ValueError(\n",
    "            f\"NMDB station not allowed: {station}. \"\n",
    "            f\"Allowed: {NMDB_ALLOWED_STATIONS}\"\n",
    "        )\n",
    "    \n",
    "    params = [\n",
    "        (\"formchk\", \"1\"),\n",
    "        (\"stations[]\", station),\n",
    "        (\"tabchoice\", \"revori\"),\n",
    "        (\"dtype\", \"corr_for_pressure\"),\n",
    "        (\"tresolution\", \"30\"),\n",
    "        (\"date_choice\", \"bydate\"),\n",
    "\n",
    "        (\"start_year\", f\"{start_dt.year:04d}\"),\n",
    "        (\"start_month\", f\"{start_dt.month:02d}\"),\n",
    "        (\"start_day\", f\"{start_dt.day:02d}\"),\n",
    "        (\"start_hour\", f\"{start_dt.hour:d}\"),\n",
    "        (\"start_min\", f\"{start_dt.minute:d}\"),\n",
    "\n",
    "        (\"end_year\", f\"{end_dt.year:04d}\"),\n",
    "        (\"end_month\", f\"{end_dt.month:02d}\"),\n",
    "        (\"end_day\", f\"{end_dt.day:02d}\"),\n",
    "        (\"end_hour\", f\"{end_dt.hour:d}\"),\n",
    "        (\"end_min\", f\"{end_dt.minute:d}\"),\n",
    "\n",
    "        (\"output\", \"ascii\"),\n",
    "        (\"yunits\", \"0\"),\n",
    "        (\"anomalous\", \"1\"),\n",
    "        (\"display_null\", \"1\"),\n",
    "    ]\n",
    "\n",
    "    # --- LOGIC ---\n",
    "    url = VAR[\"NMDB_BASE_URL\"] + \"?\" + urlencode(params, doseq=True)\n",
    "    \n",
    "    with urlopen(url, timeout=30) as resp:\n",
    "        text = resp.read().decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "    # --- RETURN ---\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data (single-file Bronze writes)\n",
    "\n",
    "This section contains the persistence utilities and “fetch + persist” entrypoints:\n",
    "\n",
    "- `write_single_file(...)`: forces **one output file** (Spark `coalesce(1)` → rename `part-*`)\n",
    "- `save_gfz(...)`: fetches via `getGFZindex(...)`, serializes to CSV, writes a single file\n",
    "- `save_nmdb(...)`: fetches via `getNMDBnest(...)`, writes raw ASCII as a single TXT file\n",
    "\n",
    "All saved filenames embed `start-YYYY-MM-DD` and `end-YYYY-MM-DD` so incremental updates\n",
    "can infer the next missing window from existing Bronze files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_single_file(dir_path: str, filename: str, content: str) -> str:\n",
    "    \"\"\"\n",
    "    Write a single text file to DBFS/ABFSS via Spark.\n",
    "\n",
    "    Spark text writes produce a directory of part-files, so this helper writes to a\n",
    "    temporary directory with `coalesce(1)`, moves the single `part-*` file to the\n",
    "    requested `{dir_path}/{filename}`, then deletes the temp directory.\n",
    "\n",
    "    Args:\n",
    "        dir_path (str): Destination directory path (DBFS/ABFSS).\n",
    "        filename (str): Target filename to create in `dir_path`.\n",
    "        content (str): Full file contents to write.\n",
    "\n",
    "    Returns:\n",
    "        str: Full path to the written file.\n",
    "\n",
    "    Raises:\n",
    "        RuntimeError: If no `part-*` file is produced in the temporary directory.\n",
    "    \"\"\"\n",
    "\n",
    "    dbutils.fs.mkdirs(dir_path)\n",
    "    target = f\"{dir_path.rstrip('/')}/{filename}\"\n",
    "\n",
    "    # Best-effort remove existing target\n",
    "    try:\n",
    "        dbutils.fs.rm(target)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Unique temp dir (avoid collisions); uses current UTC timestamp\n",
    "    tmp_tag = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "    tmp_dir = f\"{dir_path.rstrip('/')}/_tmp_{filename}_{tmp_tag}\"\n",
    "\n",
    "    # Spark write (single partition -> single part-* file)\n",
    "    df = spark.createDataFrame([(content,)], [\"value\"])\n",
    "    df.coalesce(1).write.mode(\"overwrite\").text(tmp_dir)\n",
    "\n",
    "    # Find the part file and move it to the desired filename\n",
    "    part_files = [x.path for x in dbutils.fs.ls(tmp_dir) if x.name.startswith(\"part-\")]\n",
    "    if not part_files:\n",
    "        raise RuntimeError(f\"No part-* file found in {tmp_dir}\")\n",
    "\n",
    "    dbutils.fs.mv(part_files[0], target, True)\n",
    "    dbutils.fs.rm(tmp_dir, True)\n",
    "\n",
    "    return target\n",
    "\n",
    "def save_gfz(outputdir:str, idx:str, startdate: str, enddate: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch a GFZ geomagnetic index for a UTC window and write it as a single CSV file.\n",
    "\n",
    "    Args:\n",
    "        outputdir (str): Destination directory path (DBFS/ABFSS).\n",
    "        idx (str): GFZ index name (e.g., \"Hp30\").\n",
    "        startdate (str): 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SSZ' (UTC).\n",
    "        enddate (str): 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SSZ' (UTC).\n",
    "\n",
    "    Returns:\n",
    "        str: Full path to the written CSV file.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If timestamps are invalid or enddate < startdate.\n",
    "        RuntimeError: If the output file cannot be materialized as a single file.\n",
    "    \"\"\"\n",
    "\n",
    "    start_iso = normalize_utc(startdate, is_start=True)\n",
    "    end_iso   = normalize_utc(enddate, is_start=False)\n",
    "\n",
    "    start_day = start_iso[:10]\n",
    "    end_day   = end_iso[:10]\n",
    "\n",
    "    run_tag = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "\n",
    "    dt_vals, idx_vals, _ = getGFZindex(start_iso, end_iso, idx, \"def\")\n",
    "\n",
    "    csv_lines = [f\"datetime,{idx}\"]\n",
    "    for d, v in zip(dt_vals, idx_vals):\n",
    "        csv_lines.append(f\"{d},{v}\")\n",
    "\n",
    "    filename = f\"gfz_index-{idx}_start-{start_day}_end-{end_day}_tag-{run_tag}.csv\"\n",
    "    return write_single_file(outputdir, filename, \"\\n\".join(csv_lines))\n",
    "\n",
    "\n",
    "def save_nmdb(outputdir:str, station:list,startdate: str, enddate: str) -> str:\n",
    "    \"\"\"\n",
    "    Fetch NMDB NEST data for one station over a UTC window and write it as a single TXT file.\n",
    "\n",
    "    Args:\n",
    "        outputdir (str): Destination directory path (DBFS/ABFSS).\n",
    "        station (str): NMDB station code (must be allowed by `getNMDBnest`).\n",
    "        startdate (str): 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SSZ' (UTC).\n",
    "        enddate (str): 'YYYY-MM-DD' or 'YYYY-MM-DDTHH:MM:SSZ' (UTC).\n",
    "\n",
    "    Returns:\n",
    "        str: Full path to the written TXT file.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If timestamps are invalid, enddate < startdate, or station is invalid.\n",
    "        urllib.error.URLError: If the request fails (network/DNS/timeout).\n",
    "        urllib.error.HTTPError: If NMDB returns a non-2xx HTTP status.\n",
    "        RuntimeError: If the output file cannot be materialized as a single file.\n",
    "    \"\"\"\n",
    "        \n",
    "    start_iso = normalize_utc(startdate, is_start=True)\n",
    "    end_iso   = normalize_utc(enddate, is_start=False)\n",
    "\n",
    "    start_day = start_iso[:10]\n",
    "    end_day   = end_iso[:10]\n",
    "\n",
    "    run_tag = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "\n",
    "    text = getNMDBnest(start_iso, end_iso,station)\n",
    "\n",
    "    filename = f\"nmdb_stations-{station}_start-{start_day}_end-{end_day}_tag-{run_tag}.txt\"\n",
    "    return write_single_file(outputdir, filename, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering Functions (registry)\n",
    "\n",
    "To keep orchestration generic, we register the **source-specific save functions** in `VAR['importfuncs']`\n",
    "so they can be called indirectly later (by source key), e.g.:\n",
    "\n",
    "- `save_gfz`  -> GFZ index writer\n",
    "- `save_nmdb` -> NMDB station writer\n",
    "\n",
    "This keeps the \"which source do I call?\" decision out of the orchestration logic and\n",
    "lets the run loop dispatch by `source` + `idx` (where `idx` is the GFZ index or NMDB station code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR[\"importfuncs\"] = {\n",
    "    \"gfz\":save_gfz,\n",
    "    \"nmdb\":save_nmdb,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run: bootstrap or update\n",
    "\n",
    "For each configured task:\n",
    "- if the destination folder is empty → bootstrap\n",
    "- otherwise → incremental update\n",
    "\n",
    "This keeps Bronze append-only and makes reruns idempotent at the “file per window” level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_2025tonow(importfunc,outputdir,idx):\n",
    "    \"\"\"\n",
    "    Backfill a full 2025-to-yesterday (UTC) window into Bronze for one source/index.\n",
    "\n",
    "    Computes the window `2025-01-01T00:00:00Z` to end-of-yesterday (UTC), normalizes\n",
    "    timestamps with `normalize_utc()`, then calls `importfunc(outputdir, idx, start_iso, end_iso)`.\n",
    "\n",
    "    Args:\n",
    "        importfunc (Callable): Import/write function (e.g., `save_gfz` or `save_nmdb`).\n",
    "        outputdir (str): Destination directory path (DBFS/ABFSS).\n",
    "        idx (str): Index/station identifier passed through to `importfunc`.\n",
    "\n",
    "    Returns:\n",
    "        Any: Whatever `importfunc` returns (typically the written file path).\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the computed end timestamp is before 2025-01-01.\n",
    "    \"\"\"\n",
    "    # --- window: 2025-01-01 .. yesterday (UTC) ---\n",
    "    startdate = \"2025-01-01\"\n",
    "    enddate = (datetime.now(timezone.utc) - timedelta(days=1)).date().isoformat()\n",
    "\n",
    "    # normalize once to validate + for display\n",
    "    start_iso = normalize_utc(startdate, is_start=True)\n",
    "    end_iso = normalize_utc(enddate, is_start=False)\n",
    "\n",
    "    if end_iso < \"2025-01-01T00:00:00Z\":\n",
    "        raise ValueError(f\"Computed end={end_iso} is before 2025-01-01; refusing to bootstrap.\")\n",
    "\n",
    "    return importfunc(outputdir,idx,start_iso, end_iso)\n",
    "\n",
    "def update_data(importfunc,outputdir,idx):\n",
    "    \"\"\"\n",
    "    Incrementally fetch new data since the latest saved file and append it to Bronze.\n",
    "\n",
    "    Scans `outputdir` for existing files, extracts each file's end date from the\n",
    "    filename pattern containing `end-<YYYY-MM-DD>`, then sets the next start date\n",
    "    to (latest_end + 1 day). Uses yesterday (UTC) as the update end date and calls\n",
    "    `importfunc(outputdir, idx, start_iso, end_iso)` when updates are needed.\n",
    "\n",
    "    Args:\n",
    "        importfunc (Callable): Import/write function (e.g., `save_gfz` or `save_nmdb`).\n",
    "        outputdir (str): Destination directory path (DBFS/ABFSS).\n",
    "        idx (str): Index/station identifier passed through to `importfunc`.\n",
    "\n",
    "    Returns:\n",
    "        Any | None: `importfunc` result (typically a written file path), or None if\n",
    "            no update is needed.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If timestamps or filename-derived dates cannot be parsed.\n",
    "        Exception: If listing `outputdir` or `importfunc` execution fails.\n",
    "    \"\"\"\n",
    "\n",
    "    startdate = parse_utc(\"2025-01-01\",is_start=True)\n",
    "    for file in dbutils.fs.ls(outputdir):\n",
    "        file_enddate = parse_utc(file.name.split(\"end-\")[1].split(\"_\")[0],is_start=False)\n",
    "        file_nextday = parse_utc(\n",
    "            (file_enddate + timedelta(days=1)).date().isoformat(),\n",
    "            is_start=True,\n",
    "        )\n",
    "        \n",
    "        startdate = file_nextday if file_nextday > startdate else startdate\n",
    "        \n",
    "    enddate = parse_utc(\n",
    "        (datetime.now(timezone.utc) - timedelta(days=1)).date().isoformat(),\n",
    "        is_start=False,\n",
    "    )\n",
    "\n",
    "    start_iso = startdate.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    end_iso = enddate.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    if enddate < startdate:\n",
    "        print (f\"The {outputdir} resource is uptodated to {start_iso}\")\n",
    "        return None\n",
    "    \n",
    "    return importfunc(outputdir,idx,start_iso, end_iso)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show time\n",
    "\n",
    "The code below is the **main driver** for this notebook.\n",
    "\n",
    "It iterates over `VAR[\"outputdirs\"]`, derives `source` + `idx` from the task name,\n",
    "checks whether the destination folder is empty, and then dispatches to:\n",
    "- `bootstrap_2025tonow(...)` for first-time backfills (2025 → yesterday UTC)\n",
    "- `update_data(...)` for incremental ingestion since the latest saved end date\n",
    "\n",
    "At the end it prints the written paths (bootstrap and/or update) per task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task,outputdir in VAR[\"outputdirs\"].items():\n",
    "\n",
    "    source,idx = task.split(\"_\")\n",
    "\n",
    "    try: flag_empty = len(dbutils.fs.ls(outputdir)) == 0\n",
    "    except Exception: flag_empty = True  # missing/unlistable -> treat as empty\n",
    "\n",
    "    bootstrap_message = {}\n",
    "    updated_message = {}\n",
    "    if flag_empty:\n",
    "        bootstrap_message[task] = bootstrap_2025tonow(VAR[\"importfuncs\"][source],outputdir,idx)\n",
    "    else:\n",
    "        updated_message[task] = update_data(VAR[\"importfuncs\"][source],outputdir,idx)\n",
    "    \n",
    "print(\"Bootstrap complete. Written paths:\")\n",
    "for k, v in bootstrap_message.items():\n",
    "    print(f\"- {k}: {v}\")\n",
    "\n",
    "print(\"\\nUpdate complete. Written paths:\")\n",
    "for k, v in updated_message.items():\n",
    "    print(f\"- {k}: {v}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "0_bronze",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "pylinjh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
